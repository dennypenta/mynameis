"use strict";(self.webpackChunkmynameis=self.webpackChunkmynameis||[]).push([[538],{6362:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"gopls-08","metadata":{"permalink":"/mynameis/blog/gopls-08","source":"@site/blog/08-gopls/index.mdx","title":"gopls: how your IDE becomes better every day","description":"A few years ago we had 2 editors for Go: jetbrains plugin to IntelijIdea and YOURNAMEIT with a go plugin.","date":"2025-02-15T21:41:35.000Z","formattedDate":"February 15, 2025","tags":[{"label":"software","permalink":"/mynameis/blog/tags/software"},{"label":"go","permalink":"/mynameis/blog/tags/go"}],"readingTime":5.23,"hasTruncateMarker":true,"authors":[{"name":"Denis","title":"Software Experience Dude","key":"denis"}],"frontMatter":{"slug":"gopls-08","title":"gopls: how your IDE becomes better every day","authors":["denis"],"tags":["software","go"]},"unlisted":false,"nextItem":{"title":"Go Dockerfile","permalink":"/mynameis/blog/go-dockerfile"}},"content":"A few years ago we had 2 editors for Go: jetbrains plugin to IntelijIdea and YOUR_NAME_IT with a go plugin.\\nFirst brought tons of closed source components that eventually became Goland and bunch of open source components used by another editor Go plugin:\\n\\nIt worked for a while, but it has a lot of issues.\\n\\n1. Number one, you install a plugin, it installs a tenish of the others, every version half of the tools are broken and that\'s very lame dance happened every release.\\n2. The integration complexity, every editor did its own integration set, some used only part of them, some integrated them one by one.\\n3. Performance, it\'s not obvious today, but every tool worked once per editor request, it means every click \\"Go to definition\\" it has index the code base, find the implementatons or the object locations navigate you.\\n\\n\x3c!--truncate--\x3e\\n\\n![tools](./tools.png)\\n\\n### LSP\\n\\nAnd Microsoft did a next step in creating IDE.\\nThey decided it\'s important to decouple Javascript language support from the the editor.\\nSo they created another component for autocompletion, code navigation, refactoring and diagnostic.\\nThis component must run outside of editor in order to let many vscode instances to reuse the same indexing and cache.\\nThe called it a language server and it made a new standard: LSP, Lanugage Server Protocol.\\n\\n[microsoft.github.io/language-server-protocol](https://microsoft.github.io/language-server-protocol/specifications/lsp/3.17/specification/)\\n\\n![gopls](./gopls.png)\\n\\nIt gives every language team to make a developer experience an editor independant delegating all the main editor capabilities to the LSP implementation.\\n\\nIt changes 3 fundamental things in the software history:\\n\\n- Language team supports any LSP compatible editor, it gives more editors come to the market like Helix, Zed, NVIM, etc.\\n- We finally don\'t need to install a bunch of binaries to make the development experience just \\"fine\\"\\n- And a bonus point, it gives a Go team to focus on supporting more complicated use cases like large repos support increasing performance for everyone, so it means everyone can make a dev experience better for everyone.\\n\\n### LSP Protocol\\n\\nLet\'s have a look at the protocol breifly.\\nIt has a client that drives all the necessary capabilities implemented on button clicks, hover, hotkeys, etc.\\nImplementing an LSP client it calls a server to resolve some actions.\\nFor instance clicking Go to definition it sends a request with a given editor position where the interface method is described.\\n\\n```json\\n{\\n  ...\\n  \\"method\\": \\"textDocument/definition\\",\\n  \\"textDocument\\": {\\"uri\\": \\"file://location/file.go\\"},\\n  \\"position\\": {\\"line\\": 42, \\"character: 3\\"},\\n  ...\\n}\\n```\\n\\nAnd the server contains all the indexed codebase can return a list of locations so editor suggest them to a developer.\\n\\n### gopls under the hood\\n\\nGlobally gopls has Cache.\\nOpening an editor it creates a new Session to connect to the server.\\nOpening a new project in the editor it creates a new Workspace looking for a module, go.work or GOPATH directory.\\nIt holds metadata to let gopls know where you actually want to navigate in the code.\\nIt uses default build, it also means it ignores build flags if you use them.\\n\\n:::info\\nIn case you wish to see better dev exprience:\\n\\n- changes to one module to be reflected in another\\n- optimize memory usage by reducing the number of builds it must track\\n- give gopls information where do you work\\n  then use go.work.\\n\\n  :::\\n\\n### Capabilities\\n\\nAll the features list you can find [here](https://github.com/golang/tools/blob/master/gopls/doc/features/README.md).\\nBelow I shortly describe what you could find useful to perform gopls at 100%.\\n\\n- General information and a doc string on hovering\\n- Diagnostics: static analysys, compile errors\\n- Completion: how vscode intellisense gives you suggestion or cmp/blink for nvim.\\n- Navigation: Go to definition, List references, List implementations, etc.\\n- Transformation: format, sort imports, and refactoring as rename, inline, extract, etc.\\n\\n<img src={require(\\"./goplsdemo.gif\\").default} />\\n\\n### What about Goland\\n\\nGoland is the opposite, it provides its capabilities as a closed source software. Lately it supports LSP as well, but they rarely can be compared in the feature set.\\n\\nThe main difference we all together can make gopls better, while we never can contribute to Goland.\\n\\nIf you found an idea like \\"I miss this hostkey to make this thing\\" or \\"generate that piece\\" or \\"discover that place\\" you probably want to develop another editor plugin.\\nIt happened it me either, but apparently it\'s not the best solution.\\nIf it\'s purely related to a language you just need to contribute to gopls.\\n\\n### How to debug gopls\\n\\nHere I describe a practical guide to gopls contribution.\\nI skip the issue discussion with your proposal, expected output, how you attach a visual picture and communicate nicely, gerrit user experience so on.\\n\\nAll the examples are using vscode, I\'m sure vim/emacs nerds have cracked it.\\nWe don\'t use GoLand here because it has a lot of internal tooling for Go that may add misunderstanding whether it\'s gopls or GoLand magic.\\n\\nFirst, let\'s configure a local debugging setup.\\nInside the repo we add a debugging statement for vscode in order to run our forked version of gopls, in a folder `tools/gopls/` we add:\\n\\n```json\\n{\\n  \\"version\\": \\"0.2.0\\",\\n  \\"configurations\\": [\\n    {\\n      \\"name\\": \\"Run Gopls\\",\\n      \\"type\\": \\"go\\",\\n      \\"request\\": \\"launch\\",\\n      \\"mode\\": \\"auto\\",\\n      \\"program\\": \\"./main.go\\",\\n      \\"args\\": [\\"-port=9000\\"]\\n    }\\n  ]\\n}\\n```\\n\\nNow you can run your gopls.\\n\\nIn another editor instance you add a workspace setting:\\n\\n```json\\n\\"go.languageServerFlags\\": [\\n  \\"-remote=localhost:9000\\"\\n]\\n```\\n\\nSo you know how to make a call from your editor to the server and found a place to debug.\\n\\nFor example you want to add a quickfix, it\'s a Code Action to make a Transformation, like implementing an interface or generating a function, etc.\\n\\nNow guess where to put a breakpoint and call a Code Action from a client, in vscode you need to push `CMD + .` or find your hotkey in the [doc](https://code.visualstudio.com/docs/editor/refactoring#_code-actions-quick-fixes-and-refactorings).\\n\\n### A few nice words in the end\\n\\nWhat\'s really important Go team uses LSP compatible editors, so real dog fooding comes in the room, they experience what the build and introduce the features they miss.\\nI know for sure Alan Donovan uses Emacs, at least he said so.\\n\\nIf you are still interested look his [speach](https://youtu.be/8EsaJC9cn4w) how they optimized Gopls last interation.\\n\\nA really hope it raises awareness across the entire Go community and lights the interest to the tooling, more people contribute better experience everyone would have."},{"id":"go-dockerfile","metadata":{"permalink":"/mynameis/blog/go-dockerfile","source":"@site/blog/07-go-dockerfile/index.mdx","title":"Go Dockerfile","description":"The things I\'ve collected to write my best Dockerfile. Appreciate any comments mentioning I could do it better and more optimal.","date":"2025-01-21T20:11:35.000Z","formattedDate":"January 21, 2025","tags":[{"label":"software","permalink":"/mynameis/blog/tags/software"},{"label":"go","permalink":"/mynameis/blog/tags/go"}],"readingTime":6.63,"hasTruncateMarker":true,"authors":[{"name":"Denis","title":"Software Experience Dude","key":"denis"}],"frontMatter":{"slug":"go-dockerfile","title":"Go Dockerfile","authors":["denis"],"tags":["software","go"]},"unlisted":false,"prevItem":{"title":"gopls: how your IDE becomes better every day","permalink":"/mynameis/blog/gopls-08"},"nextItem":{"title":"Unit test in observability","permalink":"/mynameis/blog/unit-test-observability"}},"content":"The things I\'ve collected to write my best Dockerfile. Appreciate any comments mentioning I could do it better and more optimal.\\n\\n\x3c!--truncate--\x3e\\n\\n## Preface\\n\\nTLDR:\\n\\n```docker\\nFROM golang:1.23.1-alpine AS builder\\n\\nWORKDIR /app\\n\\nENV CGO_ENABLED=0 \\nENV GOOS=linux\\n\\nCOPY go.mod go.mod\\nCOPY go.sum go.sum\\nRUN --mount=type=cache,target=/go/pkg/mod/ go mod download -x\\n\\nCOPY . .\\n\\nFROM builder AS dev\\n\\nRUN --mount=type=cache,target=/go/pkg/mod/ --mount=type=cache,target=\\"/root/.cache/go-build\\" go install github.com/go-delve/delve/cmd/dlv@v1.23.0\\n\\nRUN --mount=type=cache,target=/go/pkg/mod/ --mount=type=cache,target=\\"/root/.cache/go-build\\" go build -gcflags=all=\\"-N -l\\" -o server ./cmd/server\\n\\nCMD [\\"dlv\\", \\"--listen=:40000\\", \\"--continue\\", \\"--headless=true\\", \\"--api-version=2\\", \\"--accept-multiclient\\", \\"exec\\", \\"server\\"]\\n\\nFROM builder AS prod\\n\\nRUN --mount=type=cache,target=/go/pkg/mod/ --mount=type=cache,target=\\"/root/.cache/go-build\\" go build -ldflags \\"-s -w\\" -o server ./cmd/server\\n\\nFROM alpine:3.13\\n\\nRUN addgroup -g 1001 appgroup && adduser -D -G appgroup -u 1001 appuser\\n\\nWORKDIR /app\\n\\nUSER 1001\\n\\nCOPY --from=prod /app/server server\\n\\nCMD [\\"/app/server\\"]\\n```\\n\\nLink: https://github.com/treenq/treenq/blob/98e6d8dd5f5756fe5df561913e10515784ef7163/Dockerfile\\n\\nNow let\'s breakdown what\'s happening here and why.\\n\\n:::note \\nI use colima and all the things described here work well. However, all the docs referenses will go to docker. I think they did a great job to push the industry standart. Also you have to turn on buildx to make these features work. It requires buildx installation and setting a feature flag `DOCKER_BUILDKIT=1`.\\n:::\\n\\n## Base image\\n\\nFirst, we need a base image in order to build the app. We want the base being less as possible, for that purpose I use alpine:\\n\\n```docker\\nFROM golang:1.23.1-alpine AS builder\\n```\\n\\nYou also can consider [distroless](https://github.com/GoogleContainerTools/distroless), it\'s very suitable for interpreted languages like python/nodejs, but alpine works well for Go.\\n\\n## Dependencies\\n\\nThe next step we prepare a surface to build the image, all the necessary dependencies we install there.\\n\\nIf you need specific timezones, certificates, github private repo creds, the service modules, whatever, we do it right here.\\n\\n```docker\\nCOPY go.mod go.mod\\nCOPY go.sum go.sum\\nRUN --mount=type=cache,target=/go/pkg/mod/ go mod download -x\\n```\\n\\nWe must copy only desegnated dependencies definition and right after download them.\\nIf you don\'t know why and what is docker layers please refer to Docker basics and get back.\\n\\n`-x` Flag is kinda adds verbosity showing what `go mod download` executes.\\n\\nThe interesting details here is `--mount=type=cache`.\\nYou can find more in the [reference](https://docs.docker.com/reference/dockerfile/#run---mounttypecache)\\n\\nIf you update a dependency in your `go.mod` and rebuild it, then this statement will not download all the packages from scratch, it creates a designated mount and hold them in one place.\\n\\nUnfortunately, most of the CI systems create a new image for a new job and it doesn\'t work in CI, but helps a lot for local testing. For instance, I run e2e tests locally and it saves a couple of minutes every try.\\n\\nAnd only then we copy the rest of the codebase:\\n\\n```docker\\nCOPY . .\\n```\\n\\n:::note \\nDon\'t forget to check your dockerignore to skip copying useless files.\\n:::\\n\\n## Targets\\n\\nYou must have heard about multi-stage images.\\nAnd you have built the dockerfiles with 2 stages, first to build a binary and the next to run it in a blank environment.\\n\\nBut I will convince you to have 4 stages.\\n\\nDocker has an amazing feature: targets.\\n\\nThe targets allow to run a specified stage of the image.\\n\\nLet\'s have a look how it plays out in case of e2e tests.\\n\\nWe have a regular docker compose setup with a database or whatever dependencies you have to run your tests.\\nIm a big fan of a debugger and a big hater of infinite print statements. Don\'t get me wrong, logging is an awesome tool, but not the forgotten prints in a production build.\\n\\n```docker\\nFROM builder AS dev\\n\\nRUN --mount=type=cache,target=/go/pkg/mod/ --mount=type=cache,target=\\"/root/.cache/go-build\\" go install github.com/go-delve/delve/cmd/dlv@v1.23.0\\n\\nRUN --mount=type=cache,target=/go/pkg/mod/ --mount=type=cache,target=\\"/root/.cache/go-build\\" go build -gcflags=all=\\"-N -l\\" -o server ./cmd/server\\n\\nCMD [\\"dlv\\", \\"--listen=:40000\\", \\"--continue\\", \\"--headless=true\\", \\"--api-version=2\\", \\"--accept-multiclient\\", \\"exec\\", \\"server\\"]\\n```\\n\\nOn installing delve don\'t forget to specify cache mount, it has its own dependencies and it will help to speed the build up.\\nMoreover, if you don\'t mount the cache it won\'t discover the installeed dependencies in a previous step.\\n\\nThe mount flag might look unclear, as an argument you only put a path _where_ to mount, you can\'t control the source of directory, it\'s managed docker buildkit.\\n\\nThen we build a binary with a couple important gcflags, where `-l` disables inlining and `-N` removes optimisations. It matters because otherwise debugger won\'t be able to show some variables or navigate into some functions.\\nYou can read more [here](https://pkg.go.dev/cmd/compile).\\n\\nAs the last statement we run dlv.\\n\\nAnd that\'s what we have in a docker compose:\\n\\n```yaml\\nserver:\\n  build:\\n    context: .\\n    dockerfile: Dockerfile\\n    target: dev\\n  ports:\\n    - \'8000:8000\'\\n    - \'40000:40000\'\\n  security_opt:\\n    - seccomp:unconfined\\n  cap_add:\\n    - SYS_PTRACE\\n```\\n\\nHere we specify target field, clear and simple. You also can pass it as a cli argument to an image.\\n\\nPorts has 2 elements, a regular app HTTP port and a DAP (Debug Adapter Protocol) port that delve exposes.\\n\\nNext we add `secuty_opt` since the default Seccomp profile restricts the `ptrace` system call.\\n\\n:::note \\nSeccomp is a Linux kernel feature used to restrict the system calls that a process can make. By default, Docker applies a restrictive Seccomp profile to limit potentially dangerous system calls, improving container security. Read more [here](https://docs.docker.com/engine/security/seccomp/) and [here](https://docs.docker.com/engine/containers/run/).\\n:::\\n\\nWhen you specify `seccomp:unconfined`, it removes the Seccomp restrictions, allowing the container to make all system calls. This config allows running `ptrace` syscall in the container, delve uses it to set breakpoints, observe memory, etc.\\n\\nBut it\'s not enough. We have to not only remove a restriction, but explicitly give a permission, that\'s why we have `cap_add` statement: to add a capability for that syscall.\\n\\n## Build prod\\n\\nThe prod build is quite simple and well known:\\n\\n```docker\\nFROM builder AS prod\\n\\nRUN --mount=type=cache,target=/go/pkg/mod/ --mount=type=cache,target=\\"/root/.cache/go-build\\" go build -ldflags \\"-s -w\\" -o server ./cmd/server\\n```\\n\\nWe still use mount cache, we just put different build flags, in this case ldflags to achieve exactly the opposite we did in order to build a debug target.\\n`-s` and `-w` stand for skipping debug info, read more [here](https://pkg.go.dev/cmd/link).\\n\\n## Run prod\\n\\nThere is no lots of new things for you, I want to focus on a small important thing: a user.\\n\\n```docker\\nRUN addgroup -g 1001 appgroup && adduser -D -G appgroup -u 1001 appuser\\n\\nUSER 1001\\n```\\n\\nThere is so many information around on this security topic and I keep seeing zero attention to a user inside the image.\\n\\nShortly speaking - less ability less chance to make a mistake or open a vulnerability. Docker has a good blog [post](https://www.docker.com/blog/understanding-the-docker-user-instruction/) to cover why it matters.\\n\\n## Target dependenciy graph\\n\\nNow about the main concern of so many stages.\\n\\nWhy would I build all the stages for releasing my go app?\\n\\nYou will not, if you turned on buildkit it will behave as a smarter and build only the necessary dependencies, it means it builds the dependency graph and builds only necessary part, so your production CI will never install delve to waste your time.\\nThe [documentation](https://docs.docker.com/build/building/multi-stage/#differences-between-legacy-builder-and-buildkit) explains it very well.\\n\\n## A few caveats on debugging remote DAP.\\n\\nIf you start a debugging process as is you will find your breakpoints Rejected. \\nIt happens because your DAP communicates breakpoints state with DAP server using the client paths, and your client IDE is located on your machine, while the Go binary was built inside an image, another host machine.\\n\\nYou can find the fix in the official [doc](https://github.com/go-delve/delve/blob/master/Documentation/cli/substitutepath.md).\\n\\nFirst, I would connect to dlv `dlv connect localhost:40000` and test path substitution, \\nfor instance `config substitute-path /path/in/docker /local/path` where /path/in/docker is just your WORKDIR statement and /local/path is your local dir (input `pwd` in the project folder).\\nAfter that you can try `list main.main` and make sure it lists you a main function without an error.\\n\\nEventually I have the following config to configure remote debugger:\\n```json\\n{\\n      \\"type\\": \\"go\\",\\n      \\"name\\": \\"debug remote service\\",\\n      \\"mode\\": \\"remote\\",\\n      \\"request\\": \\"attach\\",\\n      \\"port\\": 40000,\\n      \\"substitutePath\\": [\\n        {\\n          \\"from\\": \\"${env:HOME}/projects/project-name\\",\\n          \\"to\\": \\"/app\\"\\n        },\\n        {\\n          \\"from\\": \\"${env:HOME}/go/pkg/mod/\\",\\n          \\"to\\": \\"/go/pkg/mod/\\"\\n        }\\n      ]\\n    }\\n```\\n\\n## Conclusion\\n\\nSorry, have nothing to say. Appreciate if you leave things better than you found it."},{"id":"unit-test-observability","metadata":{"permalink":"/mynameis/blog/unit-test-observability","source":"@site/blog/06-unit-test-observability/index.md","title":"Unit test in observability","description":"On this page, I want to cover the caveats I encountered while shaping my approach to unit testing in the context of observability.","date":"2024-10-06T21:43:35.000Z","formattedDate":"October 6, 2024","tags":[{"label":"software","permalink":"/mynameis/blog/tags/software"}],"readingTime":4.535,"hasTruncateMarker":true,"authors":[{"name":"Denis","title":"Software Experience Dude","key":"denis"}],"frontMatter":{"slug":"unit-test-observability","title":"Unit test in observability","authors":["denis"],"tags":["software"]},"unlisted":false,"prevItem":{"title":"Go Dockerfile","permalink":"/mynameis/blog/go-dockerfile"},"nextItem":{"title":"Web API Design: A Simplified Approach","permalink":"/mynameis/blog/api-design"}},"content":"On this page, I want to cover the caveats I encountered while shaping my approach to unit testing in the context of observability.\\n\\nIt\'s important to clarify that I\u2019m not referring to testing logs or metrics delivery. \\nInstead, I want to focus on testing modules that perform business logic but also include observability calls, like loggers or meters.\\n\\n\x3c!--truncate--\x3e\\n\\n## Regular Unit Test\\n\\nLet\u2019s start with a typical example of a unit test containing a logger call. \\n\\n```go\\nfunc (s *service) Do(ctx context.Context, request model.Object) (model.Response, error) {\\n\\tres, err := s.repo.Do(ctx, request)\\n\\tif err != nil {\\n\\t\\treturn res, err\\n\\t}\\n\\n\\treturn res, nil\\n}\\n```\\n\\nWith a straightforward approach, you\'d mock the repository and verify in a unit test that the mock was called with the expected parameters.\\n\\nThis is the conventional way of handling things.\\n\\n## Side Effects\\n\\nNow, let\u2019s complicate the example by adding a logger.\\n\\n```go\\nfunc (s *service) Do(ctx context.Context, request model.Object) (model.Response, error) {\\n  err := s.notImportantRepo.Something(ctx, another.Model(request))\\n  if err != nil {\\n    s.log.Error(\\"can\'t do something\\", \\"err\\", err.Error())\\n  }\\n\\n\\treturn nil\\n}\\n```\\n\\nFollowing the same approach, we can mock both dependencies. Here\'s how the mock preparation might look:\\n```go\\nnotImportantRepoErr := errors.New(\\"err\\")\\nnotImportantRepo.On(\\"Something\\", req).Return(notImportantRepoErr)\\nlog.On(\\"Error\\", mock.Anything, \\"err\\", notImportantRepoErr.Error())\\n```\\n\\nIf we make the logger call more realistic, extracting all the values we need for emergency debugging, we end up with this:\\n```go\\ns.log.Error(\\"can\'t do something\\", \\"err\\", err.Error(), \\"ctx\\", logging.FromContext(ctx), \\"another\\", mapper.From(req))\\n```\\n\\nAt this point, engineers often use mock.Anything for the logger parameters:\\n```go\\nlog.On(\\"Error\\", mock.Anything...)\\n```\\n\\nThis leads to a couple of issues:\\n- The logger call becomes a side effect that doesn\u2019t affect the result being tested.\\n- Nobody bothers to test what\'s actually logged.\\n\\n## Sustainability\\n\\nReal-world software isn\u2019t always as clean as in theory, raising a few important questions.\\n\\n1. __1.__ Does this approach improve the __quality__ of logging?\\n\\nIn some ways, yes. \\nAt least we confirm that a log message with the correct severity level was sent. \\nHowever, ensuring that the necessary parameters are logged requires strict discipline, which isn\u2019t always feasible. \\nWe can\u2019t \\"fix people.\\"\\n\\n2. __2.__ Does it __reduce PRs__ focused solely on improving logging?\\n\\nSadly, no. \\nThere\'s no validation of the parameters being logged, their mapping, formatting, or even the actual log message.\\n\\nOften, only in production do we realize, \\"Oh, it\'s missing field X.\\" After another debugging round, we might say, \\"Oh, it\'s missing field Z.\\" This cycle continues.\\n\\n3. __3.__ Does it help deliver more __observable software__?\\n\\nThis is tricky. \\nWhile it might seem like a yes (if we answer question 1), the lack of validation means we\u2019re still prone to errors. \\nThe absence of data model testing only slows down software delivery.\\n\\nEvery time an engineer touches the function, they might copy a mock from a previous test. \\nIf the function behaves differently, they\u2019ll see a failed unit test like this:\\n```\\nassert: mock: I don\'t know what to return because the method call was unexpected.\\n  Either do Mock.On(\\"Error\\").Return(...) first, or remove the Error() call.\\n  This method was unexpected:\\n    Error(params)\\n```\\n\\nThen, they\u2019ll blindly copy the previous mock statement, and the test will pass\u2014despite the logger parameters being completely different.\\n\\n## Is It Worth It?\\n\\nHere\u2019s my evaluation:\\n- Patching takes more time because of the added mock statements.\\n- More CI failures occur due to forgotten mocks.\\n\\nAll these costs offer no real benefit. \\nWe only confirm that the log was called. \\nBut for all we know, we might have sent nil to the logger and won\u2019t realize it until an emergency.\\n\\n## A Better Approach\\n\\nThere are two paths to improve this situation:\\n- Never allow loggers to accept mock.Anything.\\n- Don\u2019t test loggers in the business layer.\\n\\nThe first option is ideal but impractical unless you have years to experiment with uncertain payoffs.\\n\\nThe second option is more radical but effective: don\u2019t mix observability tests with business logic. \\nLet me explain in two steps.\\n\\n#### Nop Logger\\n\\nStart by defining the logger mock as a no-op and remove it from the mocks preparation. \\nYou can use one of the following loggers for unit tests:\\n\\n- An empty value logger\\n- A package-defined nop logger\\n- A mock logger with no assertions\\n- A logger that writes to io.Discard (no impact on unit test output)\\n\\nAny of these options will ignore logger calls and simplify development.\\n\\nYou can apply the same approach to other observability components. \\nFor example, _OpenTelemetry_ provides `\\"go.opentelemetry.io/otel/metric/noop\\"` to create an empty counter or meter.\\n\\n#### Isolation\\n\\nFinally, I\u2019m not suggesting we skip logging tests entirely.\\n\\nWe should still test them, but in __isolation__ rather than as a side effect. \\nHere\u2019s how:\\n\\n```go\\n\\tfor _, tt := range []testCase{\\n\\t\\t{\\n\\t\\t\\tname: \\"regular log\\",\\n\\t\\t\\thandler: http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\\n\\t\\t\\t\\tLoggerFromContext(r.Context()).ErrorContext(r.Context(), \\"test\\", \\"arg1\\", \\"text\\")\\n\\t\\t\\t\\tw.WriteHeader(200)\\n\\t\\t\\t}),\\n\\t\\t\\texpectedLogFunc: func(t *testing.T, m map[string]string) {\\n\\t\\t\\t\\tt.Helper()\\n\\t\\t\\t\\tassert.Equal(t, \\"ERROR\\", m[\\"level\\"])\\n        ...\\n\\t\\t\\t},\\n\\t\\t\\texpectedStatus: 200,\\n\\t\\t},\\n\\t} {\\n\\t\\tt.Run(tt.name, func(t *testing.T) {\\n\\t\\t\\tbuf := bytes.NewBuffer(nil)\\n\\t\\t\\tl := NewLogger(buf, slog.LevelInfo)\\n\\t\\t\\tm := NewLoggingMiddleware(l)\\n\\t\\t\\th := m(tt.handler)\\n\\n\\t\\t\\tw := httptest.NewRecorder()\\n\\t\\t\\th.ServeHTTP(w, httptest.NewRequest(\\"POST\\", \\"/\\", nil))\\n\\n\\t\\t\\tres := buf.String()\\n\\t\\t\\tjsonRes := make(map[string]string)\\n\\t\\t\\terr := json.Unmarshal([]byte(res), &jsonRes)\\n\\t\\t\\trequire.NoError(t, err)\\n\\n\\t\\t\\ttt.expectedLogFunc(t, jsonRes)\\n\\n\\t\\t\\tassert.Equal(t, tt.expectedStatus, w.Code)\\n\\t\\t})\\n\\t}\\n```\\n\\nInstead of writing to stderr/stdout, substitute the logger\u2019s writer with bytes.Buffer, allowing you to capture and validate the exact log message.\\n\\nThis way, we\u2019re testing the logger implementation itself, not the side effect of a function that has little to no impact on the application\'s core functionality."},{"id":"api-design","metadata":{"permalink":"/mynameis/blog/api-design","source":"@site/blog/05-api-design/index.mdx","title":"Web API Design: A Simplified Approach","description":"Note: This page won\u2019t teach you how to design an API from scratch, but it will give you insights into how I recently designed one while developing my PaaS.","date":"2024-09-19T20:45:05.000Z","formattedDate":"September 19, 2024","tags":[{"label":"software","permalink":"/mynameis/blog/tags/software"}],"readingTime":6.06,"hasTruncateMarker":true,"authors":[{"name":"Denis","title":"Software Experience Dude","key":"denis"}],"frontMatter":{"slug":"api-design","title":"Web API Design: A Simplified Approach","authors":["denis"],"tags":["software"]},"unlisted":false,"prevItem":{"title":"Unit test in observability","permalink":"/mynameis/blog/unit-test-observability"},"nextItem":{"title":"PaaS devlog |#0","permalink":"/mynameis/blog/tq-devlog-0"}},"content":"__Note__: This page won\u2019t teach you how to design an API from scratch, but it will give you insights into how I recently designed one while developing my [PaaS](https://github.com/treenq/treenq).\\n\\n\x3c!--truncate--\x3e\\n\\n## Preface\\n\\nFeel free to skip to the next section if you want to avoid the backstory.\\n\\nAfter working with various communication protocols (e.g., web form-data, REST, JSON-RPC 2.0), I\u2019ve realized that many of them aim to cover a wide range of use cases. This broad focus introduces a lot of complexity:\\n\\n- Network implementations require special handling, such as decoding domain-level errors into transport error codes and messages.\\n- Encoding and decoding messages require marshalling domain models into protocol-specific formats.\\n- Protocols like REST involve lengthy discussions about naming conventions and URL structures.\\n- Some protocols, like JSON-RPC, demand custom solutions, such as middleware for handling requests.\\n\\nThese factors complicate implementations and slow down team agreements.\\n\\n## The gRPC Shift\\n\\nWhen I started working with gRPC, it introduced some important improvements:\\n\\n- Models are generated automatically.\\n- Service interfaces are provided and generated.\\n- Clients are generated, allowing you to focus on just creating requests.\\n- Error codes are simplified, reducing them from hundreds to around 15 (which is still more than needed, but we\u2019ll get to that).\\n- The error format is predefined, so you don\u2019t have to design it\u2014just reuse [the model](https://cloud.google.com/apis/design/errors)\\n\\nAnd I liked. I don\'t know who didn\'t.\\n\\nI loved it. Most people who use gRPC feel the same.\\nHowever, gRPC doesn\u2019t naturally align with web browsers due to its reliance on HTTP/2, often necessitating additional layers to adapt to HTTP/1.1. \\nStill, browser compatibility remains a common requirement.\\n\\n## Bringing gRPC Ideas to the Web\\n\\nThis made me think: what can we adopt from gRPC for web APIs? The most obvious feature is __code generation__.\\n\\nSwagger attempts client generation but introduces its own challenges:\\n1. Swagger has too many plugins, requires specific formats, and its \\"standard\\" is fragmented. Many web renderers are incompatible with each other. For example, Apiary\u2019s requirements may contradict code generation.\\n2. Swagger focuses on defining documentation instead of the API itself, adding complexity.\\n\\nThe I discovered [tRPC](https://trpc.io/docs/quickstart), which offered many things I\u2019d been hoping for\u2014network handling as a natural consequence of defining models. \\nThe catch? It\u2019s limited to JavaScript.\\n\\nSo, I decided to design my own API approach.\\n\\n## My API Design Principles\\n\\nHere are the guiding principles I followed:\\n\\n- It must be built on top of HTTP to support the web.\\n- No URL queries, forms, or arguments in the path\u2014these overcomplicate things. Most people don\u2019t even know that URL queries are essentially [multimaps](https://en.wikipedia.org/wiki/Multimap), which leads to more bugs.\\n- Every API call uses only POST. The payload is always in the body, and for simplicity, in my case, it\u2019s JSON.\\n- The procedure naming follows the same conventions we use in code. For instance, instead of REST\u2019s `/users/{id}/wallet`, I use `/getUserWallet`.\\n\\n## What Does This Achieve?\\n\\n- I only define the models I need \u2014 no extra complexity.\\n- Optionally, I can add descriptions to make the generated documentation more talkative.\\n- From the defined router, I can generate both the client code and the API specification (including Swagger).\\n- The code generation process is simplified: a single - word URL, consistent HTTP method, and uniform marshalling strategy make everything easy to implement and maintain.\\n\\n## Error Handling\\n\\nError handling is a crucial part of any API design, and the model needs to be flexible enough to cover a wide range of scenarios. \\nBased on my experience, a simple yet effective error model looks like this:\\n\\n```go\\ntype Error struct {\\n  Code    string\\n  Message string\\n  Meta    map[string]any\\n}\\n```\\n\\n- __Code__: This holds a unique value that identifies the specific issue. \\nIt helps narrow down the location of the problem in the code and quickly leads developers to the exact line or area where the error occurred. Additionally, it provides clear guidance to users about the expected behavior or how to resolve the issue.\\n- __Message__: The message field can serve a dual purpose:\\n  1. __UserMessage__: This is a user-friendly message that can be translated into different languages on the server side. It\'s meant to be shown in the user interface to guide the user.\\n  2. __DevMessage__: This contains a technical explanation intended for developers or logs. It explains what went wrong, why it happened, and what actions can be taken to resolve it.\\nWhile you could use a single string for both, in complex applications with internationalization (i18n) needs, separating them into two distinct lines would help.\\n- __Meta__: This is a flexible map that can contain any additional context or metadata about the error. For example, if you\'re validating a form with multiple fields, the Meta map can list which fields failed validation and why.\\n\\n### Example Error Response\\n\\nHere\u2019s an example of how this error model might be used in a real-world scenario. Suppose a user submits a tax form with several invalid fields. The API can return an error response like this:\\n```go\\nError{\\n  Code: \\"TAX_FORM_INVALID\\",\\n  Meta: map[string]any{\\n    \\"tax_class\\": \\"expected a number between 1 and 4\\",\\n    \\"house_number\\": \\"must be a number\\",\\n  }\\n}\\n```\\n\\nIn this example:\\n- The __Code__ is `\\"TAX_FORM_INVALID\\"`, which indicates a validation failure specific to the tax form.\\n- The __Meta__ map provides detailed feedback on specific fields (`tax_class`, `house_number`) that need attention, making it easier for the user to correct their input.\\n\\n\\n### Error Handling in API Handlers\\n\\nIn my API, the handler interface is designed to consistently return errors using this model. \\nThis ensures that all errors adhere to a common structure, simplifying error handling on both the client and server side.\\n\\nFor instance, a typical API handler might look like this:\\n```go\\nGetProfile(ctx context.Context, _ struct{}) (GetProfileResponse, *Error)\\n```\\n\\nHere, instead of returning a plain error, the handler returns a pointer to the *Error type. \\nThis approach ensures that any error returned strictly follows the predefined error format.\\n\\n### Example from My Application\\n\\nHere\u2019s a practical example of how I\u2019ve implemented this in my app:\\n```go\\npackage domain\\n\\nimport (\\n    \\"context\\"\\n    \\"github.com/treenq/treenq/pkg/vel\\"\\n)\\n\\ntype GetProfileResponse struct {\\n    Email    string `json:\\"email\\"`\\n    Username string `json:\\"username\\"`\\n    Name     string `json:\\"name\\"`\\n}\\n\\nfunc (h *Handler) GetProfile(ctx context.Context, _ struct{}) (GetProfileResponse, *vel.Error) {\\n    profile := h.authProfiler.GetProfile(ctx)\\n    return GetProfileResponse{\\n        Email:    profile.Email,\\n        Username: profile.Username,\\n        Name:     profile.Name,\\n    }, nil\\n}\\n```\\n\\nIn this code:\\n- The `GetProfile` function fetches the user profile and returns it as a `GetProfileResponse`.\\n- The second return value is `*Error` instead of a generic `error`, making it clear that the function follows the structured error format.\\n\\n## Why Use a Custom Error Type?\\n\\nBy using a custom `Error` type instead of the standard `error`, we enforce consistency across the API. \\nThe error itself still implements Go\u2019s built-in `error` interface, allowing it to work seamlessly with existing Go error-handling patterns. \\nHowever, the benefit is that every error now carries additional structured information, like the error code and metadata, making it easier to debug and handle errors programmatically.\\n\\n## Epiloge\\n\\nI ended up building a \\"framework\\" in Go on top of net/http that is able to generate clients for any language.\\n\\nYou can check it out [here](https://github.com/treenq/treenq/blob/main/pkg/vel/router.go). \\n\\nI don\u2019t recommend using it as-is \u2014 it\u2019s better to copy and adapt it for your own needs."},{"id":"tq-devlog-0","metadata":{"permalink":"/mynameis/blog/tq-devlog-0","source":"@site/blog/04-tq-devlog-0/index.md","title":"PaaS devlog |#0","description":"Devlog #0: logging, linter, Zitadel, codegen, e2e tests","date":"2024-09-02T20:12:28.000Z","formattedDate":"September 2, 2024","tags":[{"label":"paas","permalink":"/mynameis/blog/tags/paas"}],"readingTime":8.43,"hasTruncateMarker":true,"authors":[{"name":"Denis","title":"Software Experience Dude","key":"denis"}],"frontMatter":{"slug":"tq-devlog-0","title":"PaaS devlog |#0","authors":["denis"],"tags":["paas"]},"unlisted":false,"prevItem":{"title":"Web API Design: A Simplified Approach","permalink":"/mynameis/blog/api-design"},"nextItem":{"title":"Backend Guy Ventures into Game Development","permalink":"/mynameis/blog/do-gamedev"}},"content":"## Devlog #0: logging, linter, Zitadel, codegen, e2e tests\\n\\nToday I want to share with your my first steps of creating new project.\\nFor a long time I\'ve wanted created something cool, really meaninful, and after all I step into my idea: Platform as a service.\\n\\n\x3c!--truncate--\x3e\\n\\nThe main tech stack has beeen defined: Go for a main backend, perhaps JavaScript for cdk integration.\\nAll the infra will be used from a well known cloud providers (not aws ofc, no clue who can understand how to user it).\\nFirst iteration will not such words as a frontend, ui, design. Ofc I want to make it, but only future history can judge me.\\n\\n### The problem\\n\\nThe technical problem exists, it\'s fun to figure out.\\nBut no the main one. The biggest fight Im gonna accept is __procrastination__. Im such a person who pushes further all the tasks very often. I just have little to do with time discipline. That\'s what will stop me from the progress.\\nSo the easiest way to move progress away - do whatever, but not the progress.\\n\\n### The planing\\n\\nSo I decided to plan everything and understand how big the actual project is gonna be.\\nSo I start speaking with GPT in order to express my mind, kinda talking to a duck. \\nInstead of asking what I should do I explained my plan and asked to ask __more questions__ so it could help me to clarify the technical solution and the plan.\\n\\n![img](./gpt.png)\\n\\n<details>\\n<summary>\\nHere you can find a prompt I used\\n</summary>\\nI want to develop a platform like digitalocean app, vercel, render, fly.io, heroku, etc. My value is the following: I give observability with 0 code changes, and a localdev opportunity with connecting to a cloud environment, for instance a request comes from a frontend app on staging to my backend app locally so I could intercept it and make my laptop appear kinda in the cloud. It\'s a multitenant project. There are 2 paths: kubernetes and a custom architecture. To build a custom architecture the app must be on a few virtual machines, a load balancer on top including firewall. The challenge: ask me necessary questions so we could make a right decision. I have to highlight, we build a users\' environment, not the product architecture I will create in order to reproduce the given environment, so we need to decide that would fit me and users better.\\n</details>\\n\\nInitially I thought I will do it providing a user set of virtual machines connected under VPC, putting a load balancer on top, back to 2000. I thought it will make it cheaper and some tools just easier to implement.\\n\\nChatting to GPT I realized - Kubernetes will provide it a way quicker, most of the stuff is just ready to go like network policies, load balancing, SSL, deployment/rollback, health monitoring, resources observability, Istio adoption. Istio is very useful for observability and local first development, it became a new word today - __remocal__.\\n\\nWhen stepped into the planning I decided to make it even longer.\\nI wanted to researched all the kanban boards, unresistable.\\nI used to use Trello and it was ok, but using api in power-ups boundaries sucks. Not really somethign I need right now though. \\n\\nI tried Cluckup, it\'s a laggish joke, even worse than Jira to make you feel you are already enterprise.\\nI tried Asana and saw 0 difference with Trello, I spent about 2 minutes to confirm it and exist.\\n\\n### The ugly thing\\n\\nIn the end I found Linear. \\nTo be fair it looked ugly, the design is equal to supabase: fonts, buttons.\\n\\nA few minutes later I started catching.\\nKeyboard first design - that\'s what I buy.\\n\\nSo I read Linear Method, the framework they apply internally. It was hard to accept it, I think older I get more conservatibe I become, but I dig it anyway.\\nThe basic element is an issue. It might have sub-issues. And they might have.\\nThey can be collected in a project.\\n\\nThere are bunch of other staff I don\'t use, but I will share it below.\\n- __Cycle__ - like a sprint, hate it.\\n- __Triage__ - inbox, it\'s a boss feature in my opinion, you can aggregate communication channels in a single place, then an on-call person can carry them, create issues or remove.\\n- __Initiative__ - collection of the projects, they can be presented as a time line or a strategy decision, or even a roadmap on a timeline. \\n\\nAs a result I cooked quite a few cards (48 projects in a backlog \ud83d\ude31\ufe0f\ufe0f\ufe0f\ufe0f\ufe0f\ufe0f).\\n\\n![img](./few.png)\\n\\n## Infra preparation\\n\\n### Logging and recovery\\nI implemented a simple logging solution combined with a recovery middleware. \\nI find logging panic behaviors essential, and instead of separating these middlewares, I combined them for simplicity.\\n\\nThere are several ways to handle logging in Go applications:\\n- Inject a logger into every struct\\n- Define a global (singleton) logger\\n- Inject a logger into `context.Context`\\n\\n__I prefer the last approach__ because it offers the flexibility to:\\n- Add request IDs to log messages, useful when logging in the data layer\\n- Attach additional data fields to the logger for use in middleware, e.g. computed properties in a business layer\\n\\nThe implementation is straightforward, and you can find the code [here](https://github.com/treenq/treenq/blob/21dd255da33b7255ac1338edf75b36d048a6b2b7/pkg/vel/log/log.go#L72)\\n\\nA couple of _caveats_ I encountered:\\n1. There\u2019s a timestamp formatting issue in the slog package. It sometimes fails to format correctly, so I added a simple fix:\\n```go\\nif a.Value.Kind() == slog.KindTime {\\n  t := a.Value.Time()\\n  a.Value = slog.StringValue(t.Format(time.RFC3339))\\n}\\n```\\n2. I wanted to adjust the log level based on the HTTP response status: log warnings for client errors (4xx) and errors for server errors (5xx):\\n```go\\nlogFunc := logger.DebugContext\\nif resp.status >= 500 {\\n  logFunc = logger.ErrorContext\\n```\\n\\n3. Finally, I added panic recovery and logged the error:\\n```go\\ndefer func() {\\n  if recovered := recover(); recovered != nil {\\n    logger.ErrorContext(\\n      r.Context(), \\"recovered from panic\\",\\n      ...\\n      \\"recovered\\", recovered,\\n      \\"stack\\", string(debug.Stack()),\\n    )\\n    resp.WriteHeader(http.StatusInternalServerError)\\n    if _, err := resp.Write([]byte(\\"internal server error\\")); err != nil {\\n      logger.ErrorContext(r.Context(), \\"failed to write response\\", \\"error\\", err)\\n    }\\n  }\\n}()\\n```\\n\\nIt\u2019s crucial to override the HTTP status during recovery, which is why I had to implement my own response writer.\\n\\n### Linter\\nI kept my linter setup as simple as possible. \\nI don\u2019t use a complex configuration\u2014just a single command added to the Makefile:\\n```go\\ngo install github.com/golangci/golangci-lint/cmd/golangci-lint@v1.59.1\\n```\\n\\n### AuthZ\\n\\nMy goal was to define the simplest authentication solution, one that could be extended later to support role-based access control (RBAC).\\n\\nI considered a few options, including __Ory__ and __Keycloak__, but both felt like overkill for my needs\u2014too complex and cumbersome. \\nI wanted something:\\n- Easy to configure\\n- A single binary to allow self-hosting\\n- With a simple, clear open-source license (like Apache 2.0)\\n- Dependent only on PostgreSQL\\n\\nI initially looked at __Logto__, a fresh new solution that seemed simple and clean. \\nHowever, it felt geared more toward frontend developers rather than backend engineers, and lacked some of the features I needed.\\n\\nThat\u2019s when I found __Zitadel__. \\nCreated by some of the folks behind Ory, __Zitadel__ has strong security foundations. \\nSince I\u2019m building only a backend API, I focused on authZ, using token introspection (i.e., validation), rather than issuing tokens.\\n\\nIt was straightforward to create a service account token for my e2e tests to check the authorization middleware.\\n\\nI also verified that configuring RBAC and identity brokering with providers like Google and GitHub wouldn\u2019t be an issue (i.e., \u201cSign in with Google\u201d).\\n\\n###  Generate api client\\n\\nAs I explain in my API design [page](https://dennypenta.github.io/mynameis/blog/api-design) one of the benefits I want to achieve is easy API client generation for multiple languages. \\nThe first language, naturally, is Go, as I need it for my e2e tests. \\nSince the implementation will change frequently, I\u2019m skipping unit tests for now and focusing on e2e tests to ensure the app\u2019s overall stability.\\n\\nThe client generation involves:\\n- Collecting [API definition](https://github.com/treenq/treenq/blob/21dd255da33b7255ac1338edf75b36d048a6b2b7/pkg/vel/router.go#L124) from the router (models, routes)\\n- [Building](https://github.com/treenq/treenq/blob/21dd255da33b7255ac1338edf75b36d048a6b2b7/pkg/vel/gen/client.go#L35) the data from the definitions to use in the client generation.\\n- Feeding that data into a [template](https://github.com/treenq/treenq/blob/21dd255da33b7255ac1338edf75b36d048a6b2b7/pkg/vel/gen/templates/client.tpl#L1)\\n\\nThe result is a generated Go [client](https://github.com/treenq/treenq/blob/21dd255da33b7255ac1338edf75b36d048a6b2b7/client/client.go#L11). I can easily add other templates later, such as for TypeScript, and use them in my tests.\\n\\n### Defining e2e tests\\n\\nWith the generated client, I can now define the simplest setup for my e2e tests.\\n\\nFirst, I created a Docker image and a Docker Compose setup. \\nThese are similar to a regular production environment but with a few additional tricks.\\n\\nMy [Dockerfile](https://github.com/treenq/treenq/blob/21dd255da33b7255ac1338edf75b36d048a6b2b7/Dockerfile#L15) includes an extra [target](https://docs.docker.com/build/building/multi-stage/#stop-at-a-specific-build-stage) to run a debugger in the tests:\\n```js\\nFROM builder AS dev\\n\\n# Install Delve (debugger)\\nRUN --mount=type=cache,target=/go/pkg/mod/ --mount=type=cache,target=\\"/root/.cache/go-build\\" go install github.com/go-delve/delve/cmd/dlv@v1.23.0\\n\\nRUN --mount=type=cache,target=/go/pkg/mod/ --mount=type=cache,target=\\"/root/.cache/go-build\\" go build -gcflags=all=\\"-N -l\\" -o server ./cmd/server\\n\\nCMD [\\"dlv\\", \\"--listen=:40000\\", \\"--continue\\", \\"--headless=true\\", \\"--api-version=2\\", \\"--accept-multiclient\\", \\"exec\\", \\"server\\"]\\n```\\n\\nNext, I updated my [Docker Compose](https://github.com/treenq/treenq/blob/21dd255da33b7255ac1338edf75b36d048a6b2b7/docker-compose.e2e.yaml#L19) file to expose the debugger port and allow clients to attach:\\n```yaml\\nserver:\\n  build:\\n    context: .\\n    dockerfile: Dockerfile\\n    target: dev\\n  ports:\\n    - \'8000:8000\'\\n    - \'40000:40000\'\\n  security_opt:\\n    - seccomp:unconfined\\n  cap_add:\\n    - SYS_PTRACE\\n  env_file:\\n    e2e.env\\n  depends_on:\\n    postgrese2e:\\n      condition: service_healthy\\n  restart: always\\n```\\n\\nI also created a dedicated PostgreSQL instance for e2e tests to keep it separate from my local development environment:\\n```yaml\\n  postgrese2e:\\n    image: postgres:16.3\\n    restart: always\\n    environment:\\n      POSTGRES_HOST_AUTH_METHOD: \'trust\'\\n      POSTGRES_DB: tq\\n    ports:\\n      - \\"5432:5432\\"\\n    tmpfs:\\n      - /var/lib/postgresql/data\\n    healthcheck:\\n      test: [\\"CMD-SHELL\\", \\"pg_isready -U postgres -d tq\\"]\\n      interval: 3s\\n      timeout: 3s\\n      retries: 10\\n```\\n\\n\\nFinally, I defined a simple Makefile command to run the tests:\\n```js\\nstart-e2e-test-env:\\n\\tdocker-compose -f docker-compose.e2e.yaml up -d --build\\n\\t@echo \\"Checking e2e test environment is running...\\"\\n\\tuntil $$(curl --output /dev/null --silent --fail http://localhost:8000/healthz); do printf \'.\'; sleep 1; done && echo \\"Service Ready!\\"\\n\\techo \'Service has been started\'\\n```\\n\\nThe `curl` loop waits for the service to be ready since the app depends on the database connection, and migrations need to be applied. \\nThis kind of delay is common \u2014 you can imagine even more delays due to cache warming, downstream API fetching, etc.\\n\\n#### Bye\\n\\nThank you for reading, come here and see what\'s gonna be in the end.\\n\\nRepository is here: http://github.com/treenq/treenq"},{"id":"do-gamedev","metadata":{"permalink":"/mynameis/blog/do-gamedev","source":"@site/blog/03-do-gamedev/index.mdx","title":"Backend Guy Ventures into Game Development","description":"In order to be engaged one explores different areas and looks for unknown software area.","date":"2024-09-02T18:32:54.000Z","formattedDate":"September 2, 2024","tags":[{"label":"software","permalink":"/mynameis/blog/tags/software"}],"readingTime":6.53,"hasTruncateMarker":true,"authors":[{"name":"Denis","title":"Software Experience Dude","key":"denis"}],"frontMatter":{"slug":"do-gamedev","title":"Backend Guy Ventures into Game Development","authors":["denis"],"tags":["software"]},"unlisted":false,"prevItem":{"title":"PaaS devlog |#0","permalink":"/mynameis/blog/tq-devlog-0"},"nextItem":{"title":"250mb json in a 40mb service limit","permalink":"/mynameis/blog/big-json"}},"content":"In order to be engaged one explores different areas and looks for unknown software area.\\n\\nHere we look at the gamedev in 2024 from experience in server side software.\\n\\nIn software development, it\'s common to explore new domains to stay engaged. \\nWhether it\'s for a change of pace or to push personal boundaries, exploring the unknown can lead to fancy adventures in software. \\nThis page will share my journey as a backend developer diving into the game development in 2024.\\n\\n\x3c!--truncate--\x3e\\n\\n#### Why Gamedev? \\n\\n99% of programmes came here to make a game.\\nEither did I.\\n\\nFrom a young age, I was captivated by two genres: strategy games and action RPGs. \\nMy first PC came preloaded with classics like Red Alert 2, Heroes of Might and Magic 3, and Diablo 2. \\nThe simple yet addictive gameplay\u2014click, move, destroy, enjoy \u2014 cemented my love for these genres. \\nOver the years, I\u2019ve played many iconic RTS games like Warcraft 3 and Starcraft 2 (and still do occasionally).\\nBut I realized that Starcraft 2 and similar games leaned more towards real-time execution rather than strategic depth.\\n\\nSo I decided to make my own game with the strategy and decision making take the center stage.\\n\\n#### Choosing the Game Engine \\n\\nThere are numerous options out there\u2014Defold, RPG Maker, GameMaker, Construct, to name a few.\\n\\nBut I focused on the big three that are free to start with, offer extensive flexibility, and have been tested by major projects: Unreal Engine, Unity, and Godot.\\n\\n#### Unreal Engine: A Mixed Bag of Power and Complexity\\n\\nI chose to start with Unreal Engine. It has a large community, though not the largest. Everything Unity offers in its asset store, Unreal has an equivalent. The engine supports C++ for low-level programming and Blueprints (a visual scripting language) for level design, though you can use either approach.\\n\\nInstalling the engine is a hassle. You need to first install the Epic Games Launcher and then download the 55GB engine itself, which can be a chore if your disk space is limited.\\n\\nAfter setting up a blank project, I was impressed by the vast flexibility Unreal offers. You get all the benefits of C++\u2014dynamic dependency injection, custom components, and more.. But it adds complexity.\\n1. Compiliation time. It exists. Not just exists like a couple blinks 5-10 seconds, it takes a lot of time to build a single class to start using it in the project.\\n2. Code complexity. Write header files, manual memory management. Good package manager doesn\'t exist in 2024, now I understand why they love Rust so much. \\n3. Poor mac support. No comments, even the interface of top bar menu is different, I was struggling to find editor settings according to the documentation, and every question is like that. \\n\\nI would recommend such engine for people who likes waiting, c++ and want to make a beatiful scenes with nice action. \\nI also have to note that engine is open source and owned by a technical guy and people who actually make games. I would stop there if I liked waiting and c++.\\n\\n#### Unity: The Industry Standard.\\n\\nNext up was Unity, the most popular game engine out there. \\nUnity has a vast ecosystem, with advanced features like rendering, post-processing, and easy shader creation. \\nIt\u2019s backed by a huge asset store, which should theoretically make development easier.\\nC# - Im not a big fun, but it\'s compilable, therefore type safe, seems good.\\n\\nA basic abstraction is an object. An object has compenents to manage what is it, it might be a script to move it, a mesh to render a tree, a colission not to pass through the tree, sound to play as tree leafs on the wind and so on.\\nSeems quite simple. \\n\\nBut when I approached a real problem there are plenty of weird things.\\n1. Unity has three different render pipelines, which aren\u2019t compatible with each other. The oldest one is being deprecated, meaning half the assets in the store may soon be obsolete.\\n2. Asset store is a garbage. I tried several camera controllers, and only one partially met my needs. I had to patch it to double its functionality, which felt like reinventing the wheel.\\n\\nWe all remember the Unity pricing drama. It has a closed-source codebase, and now there\u2019s a tracker that counts installations to generate more revenue for shareholders. \\nThis has led to a lot of half-baked features and outdated documentation, which can make the engine feel unstable.\\n\\nUnity is still a solid choice, but in 2024, it feels like the engine is in a weird transitional state. \\nThe documentation isn\u2019t as up-to-date as it should be, and the engine itself seems to be in a half-finished state. \\nI might revisit it in 2025 when Unity 6 is released.\\n\\n#### Godot: Unknown creature in the Sea.\\n\\nFinally, I turned to Godot, an engine I initially underestimated. \\nKnown for its strong 2D capabilities, Godot seemed lacking in 3D support. \\nIts physics engine was also rumored to be subpar.\\n\\nSo I start doing a 3D. \\nFirst I found that asset library is poor, camera controller only for Godot 3 (previous release) and it makes no sense to use Godot 3 in 2024.\\n\\nWhat I actually see in progress.\\n0. Godot\u2019s installation bundle is just 150MB. It has a negligible memory footprint and can run smoothly even on modest hardware. Coming from Unity, this was a breath of fresh air.\\n1. Godot\u2019s editor can be extended in a similar manner to Unity, allowing you to add new UI elements and dynamically instantiate new components.\\n2. The asset library isn\u2019t a store but a repository where everything is distributed for free. Since Godot is open-source, I doubt any significant part of it will go paid, unlike Unity and Unreal, where quality assets often come with a price tag.\\n3. Physics engine implementation can be replaced with jolt easy-peasy and works very well.\\n4. The same abstractions, the same component building, but not an object has component, rather a scene (actually a node) has nodes (the others to implement actual behavior of the node).\\n5. Exporting is just a button click. I managed to build a project for windows in Mac, it\'s so good, haven\'t had such smooth exprerience.\\n\\nIt was surprisingly easy to understand the basics, the nodes design, the event bus system, for a backend guy it\'s perfect to start making a real game.\\n\\nGDScript can fail in runtime, not the best, but you can use C# for the sake of compilte time failure.\\n\\nAnd I know, the community is smaller, it will be developed slower, true. But I care about me today, not the community.\\n\\nI initially planned to give Unity a fair shot, spending a week with both engines. \\nBut after just two days with Godot, I was progressing so much faster that I decided it wasn\u2019t worth the time to continue with Unity. \\nGodot\u2019s lower entry barrier and faster development pace made it the clear choice for me.\\n\\n##### I want to finish simpler. \\n\\nIn the end, the best engine is the one that aligns with your needs and preferences. \\nRemember, Blender was once considered a niche tool while everyone used 3D Max, and now Blender is the industry standard. \\nFirebase was just a \u201cbase,\u201d and today we have Supabase, Pocketbase and many others.\\n\\nThe landscape of game development is always changing. \\nWhat matters most is finding the tool that helps you bring your vision to life, not what the industry standard is. \\nFor me, that tool is Godot.\\n\\n### Lessons\\nSpoiler, no, I didn\'t finish a game, I learned the gamedev abstractions and played with the engines a bit and I enjoyed it.\\nAnd it\'s fine, I had good time and wish everyone to find a toy you can spend some time on and make you engaged."},{"id":"big-json","metadata":{"permalink":"/mynameis/blog/big-json","source":"@site/blog/02-250mbjson-in-50mb-limit/index.mdx","title":"250mb json in a 40mb service limit","description":"This article has been created to remind us of one simple thing: HTTP is a stream.","date":"2024-05-26T15:48:12.000Z","formattedDate":"May 26, 2024","tags":[{"label":"software","permalink":"/mynameis/blog/tags/software"}],"readingTime":6.8,"hasTruncateMarker":true,"authors":[{"name":"Denis","title":"Software Experience Dude","key":"denis"}],"frontMatter":{"slug":"big-json","title":"250mb json in a 40mb service limit","authors":["denis"],"tags":["software"]},"unlisted":false,"prevItem":{"title":"Backend Guy Ventures into Game Development","permalink":"/mynameis/blog/do-gamedev"},"nextItem":{"title":"Why this page exists","permalink":"/mynameis/blog/why-a-blog"}},"content":"This article has been created to remind us of one simple thing: HTTP is a stream.\\n\\nAs a practical outcome we can learn how to reduce memory requirements for our services in a typical task: cache warming.\\n\\n\x3c!--truncate--\x3e\\n\\nLet\'s look at the challenge first.\\n\\nWe have a service that must download the data and keep it in memory.\\nThe issue is the JSON document we have to download is 10 times larger than the encoded data. \\nTherefore we have to increase the memory limit 2-3 times to download it once. \\nLater on, the service doesn\'t consume as much memory, so it\'s a start up cost.\\n\\n\\n### The challenge: cut down the memory consumption as much as we can.\\n\\nLet\'s get back to the basic of network communication.\\n\\n:::note\\nWe skip TLS termination for the sake of simplicity.\\n:::\\n\\nThere is a great book that explains it very well: https://hpbn.co/building-blocks-of-tcp/#slow-start\\n\\n![img](./syn.svg)\\n\\nJust a litle picture to remind us how a connection starts: we do a handshake with the service.\\n\\nThen we can start exchanging data.\\nTypical API responses are at most ~50kb.\\n\\nBut what if you want to warm a cache? How much can it be? \\nIt can be a lot, around tens of megabytes.\\nIn my example, we take 250mb.\\n\\nHow does the server send such data?\\n\\n![img](./congestion.svg)\\n\\nSlowly, packet by packet.\\n\\nThe server tries to understand your throughput. The protocol itself rarely provides an accurate value of a packet size, so by relying on the imperical latency, it tunes the packet size little by little.\\nIt needs to send a lot of packets to transfer a really big response.\\n\\n### 3 Ways to do it\\n\\nBelow we will consider 3 approaches to solve this task.\\nThere is no such thing as the only right solution; all of them are fine as long as you understand the costs and risks well enough, and we are gonna cover them.\\n\\n### First, Brute Force solution.\\n\\n:::warning\\nIf you want to reproduce an example make sure to untar server/json.tar.gz; it must contain the f.json file since GitHub has a limit of up to 100mb for a file.\\n:::\\n\\nYou can imagine how the simplest Go HTTP client can implement it or just look at the code.\\n\\n[Link](https://github.com/dennypenta/http-response-lab/blob/543510947c0b19dbc0097adf403ae5cd6954c1cc/client/main.go)\\n\\nThe implementation is straight forward: it sends a request, gets a response, reads, marhsals it into a defined structure, holds it in the memory and ready to serve it further.\\n\\nAnd here is the pprof output:\\n\\n```shell\\nShowing nodes accounting for 229.48MB, 100% of 229.48MB total\\n      flat  flat%   sum%        cum   cum%\\n  229.48MB   100%   100%   229.48MB   100%  io.ReadAll\\n         0     0%   100%   229.48MB   100%  main.main\\n         0     0%   100%   229.48MB   100%  runtime.main\\n```\\n\\nYes, it\'s not real win, with huge memory consumption, but it works ok.\\n\\nWe see all the memory consumed on reading the HTTP stream.\\n\\nOr you might say, \\"What a noob, you must use `json.Decoder`\\" so as to let the decoder work with the HTTP pipe closer.\\n\\nAnd it\'s pretty much the same, in my example, even worse.\\n[Link](https://github.com/dennypenta/http-response-lab/blob/b6ee7fcfd69fdffad844eb6a3d324d2fe3040985/client/main.go) to code with Decoder\\n\\n```shell\\nShowing nodes accounting for 384MB, 100% of 384MB total\\n      flat  flat%   sum%        cum   cum%\\n     384MB   100%   100%      384MB   100%  encoding/json.(*Decoder).refill\\n         0     0%   100%      384MB   100%  encoding/json.(*Decoder).Decode\\n         0     0%   100%      384MB   100%  encoding/json.(*Decoder).readValue\\n         0     0%   100%      384MB   100%  main.main\\n         0     0%   100%      384MB   100%  runtime.main\\n```\\n\\nTo recall why let\'s dig a little into the json/encoding library [implementation](https://cs.opensource.google/go/go/+/refs/tags/go1.22.3:src/encoding/json/stream.go;l=49). \\n\\n```go\\nfunc (dec *Decoder) Decode(v any) error {\\n\\tif dec.err != nil {\\n\\t\\treturn dec.err\\n\\t}\\n\\n\\tif err := dec.tokenPrepareForDecode(); err != nil {\\n\\t\\treturn err\\n\\t}\\n\\n\\tif !dec.tokenValueAllowed() {\\n\\t\\treturn &SyntaxError{msg: \\"not at beginning of value\\", Offset: dec.InputOffset()}\\n\\t}\\n\\n\\t// Read whole value into buffer.\\n\\tn, err := dec.readValue()\\n\\tif err != nil {\\n\\t\\treturn err\\n\\t}\\n\\tdec.d.init(dec.buf[dec.scanp : dec.scanp+n])\\n\\tdec.scanp += n\\n\\n\\t// Don\'t save err from unmarshal into dec.err:\\n\\t// the connection is still usable since we read a complete JSON\\n\\t// object from it before the error happened.\\n\\terr = dec.d.unmarshal(v)\\n\\n\\t// fixup token streaming state\\n\\tdec.tokenValueEnd()\\n\\n\\treturn err\\n}\\n```\\n\\nIt does exactly the same, it calls `dec.readValue()` first to read all the response and then `dec.d.unmarshal` to parse it.\\n\\nAnd it\'s ok; the reason is very simple: **encoding/json doesn\'t know the nature of your data.**\\n\\n:::note\\nLibraries like [json-iter](https://github.com/json-iterator/go) or [easyjson](https://github.com/mailru/easyjson) offer zero improvements in memory consumption.\\n:::\\n\\n### Second, decode object by object.\\n\\nGo json library provides a method of the Decoder called [`Token`](https://pkg.go.dev/encoding/json#Decoder.Token)\\n\\nThis approach, parsing manually token by token, can give us an option to manually parse the json.\\nA token might be every symbol, such as as open bracket, quote, key, value, etc.\\nBut I found this approach quite complex. Having a deeply nested JSON object makes it very confusing to understand the relation for a given token. An solution could be to hold every key and designated level, but the decision gets worse with duplicated keys on a couple of levels.\\n\\nThat\'s why I prefer another approach.\\n\\nThere is a well-known problem on LeetCode called \\"[Valid Parentheses](https://leetcode.com/problems/valid-parentheses/description/)\\"\\n\\nWe can simply read the beginning of a given object and the end, understanding when the last bracket of the object comes.\\n\\n[Link](https://github.com/dennypenta/http-response-lab/blob/b5890cbb74282416b5adacc92de95f18f7ee766f/client/main.go#L110)\\n\\npprof gives the following output\\n```shell\\n(pprof) top 5\\nShowing nodes accounting for 68.56MB, 100% of 68.56MB total\\nShowing top 5 nodes out of 10\\n      flat  flat%   sum%        cum   cum%\\n   27.55MB 40.18% 40.18%    68.56MB   100%  main.decode\\n      20MB 29.17% 69.35%       20MB 29.17%  encoding/json.(*decodeState).literalStore\\n      20MB 29.17% 98.52%       20MB 29.17%  bufio.NewReaderSize (inline)\\n    1.01MB  1.48%   100%     1.01MB  1.48%  bufio.(*Scanner).Text (inline)\\n         0     0%   100%       20MB 29.17%  encoding/json.(*decodeState).object\\n```\\n\\nUsually, the output varies between 65-80mb. \\n\\nSuch adventages is achieved due to marshalling the json **and** reading the HTTP response at the same time.\\n\\nLet\'s get back to the introduction. Such a huge response gives us a stream of HTTP chunks we read step by step until the FIN message comes.\\nWe can\'t make the HTTP server split every object in the response for us (probably we can, but it brings even more complexity).\\nInstead, every given chunk window we can ask, \\"Does it contain a valid json object?\\"\\n\\nAs soon as a valid object has come, we can marshal it and continue reading the response further **until the next valid JSON object comes**.\\n\\n### Third, the simplest: data compression.\\n\\nIt was new to me to discover that the most efficient solution will nott be related to the response handling.\\n\\n**We simply must transfer as little data as we can.**\\n\\nJSON is not the only way to represent the data.\\nIt\'s easy and human-readable, but sometimes we have to trade it.\\n\\nThere are plenty of formats we can apply:\\n- Avro\\n- Tthrift\\n- MessagePack\\n- Gob (Go only)\\n- Protobuf\\n\\nAnd most likely more I don\'t even know.\\n\\nI tried replacing decoding to MessagePack and gave very litle result (zero \\\\_(\u30c4)_/).\\n\\nThe best outcome showed Protobuf.\\n:::note\\nWe still use HTTP/1.1; we don\'t use gRPC transport.\\n:::\\n\\nThe output from pprof is even better with less effort to implement.\\nIt may vary up to 50mb sometimes.\\n\\n```shell\\n(pprof) top5\\nShowing nodes accounting for 30.79MB, 100% of 30.79MB total\\n      flat  flat%   sum%        cum   cum%\\n   30.79MB   100%   100%    30.79MB   100%  io.ReadAll\\n         0     0%   100%    30.79MB   100%  main.decode\\n         0     0%   100%    30.79MB   100%  main.main\\n         0     0%   100%    30.79MB   100%  runtime.main\\n```\\n\\nHere is the solution: [Link](https://github.com/dennypenta/http-response-lab/blob/main/client/main.go#L14)\\n\\nWhat we can say about Gob? \\n\\nIt offers very specific decoding and is a Go-only implementation, but it doesn\'t provide any benefit, here is the pprof output\\n```shell\\n(pprof) top5\\nShowing nodes accounting for 70.81MB, 100% of 70.81MB total\\nShowing top 5 nodes out of 20\\n      flat  flat%   sum%        cum   cum%\\n   30.53MB 43.12% 43.12%    30.53MB 43.12%  internal/saferio.ReadData\\n   28.28MB 39.94% 83.05%    28.28MB 39.94%  reflect.growslice\\n      12MB 16.95%   100%       12MB 16.95%  encoding/gob.decString\\n         0     0%   100%    70.81MB   100%  encoding/gob.(*Decoder).Decode\\n         0     0%   100%    70.81MB   100%  encoding/gob.(*Decoder).DecodeValue\\n```\\n\\nPerhaps for someone, having schemaless implementation is valuable, so here is the solution [link](https://github.com/dennypenta/http-response-lab/blob/main/client/main.go#L40)\\n\\n\\n### Conclusion\\n\\nI found 2 interesting ideas to me during the investigation.\\n1. The best, or one of them, solution might be the most obvious, so obvious to one is not to everyone.\\n2. It\'s not hard to implement and dig into fundamentals; some may win from engineering a new bicycle."},{"id":"why-a-blog","metadata":{"permalink":"/mynameis/blog/why-a-blog","source":"@site/blog/01-why-you-need-a-blog/index.mdx","title":"Why this page exists","description":"Blog is a\xa0 system to spread ideas across the internet. Shout out about a thing \\"Look, I have an opinion on that if you care\\".","date":"2023-12-13T21:25:34.000Z","formattedDate":"December 13, 2023","tags":[{"label":"career","permalink":"/mynameis/blog/tags/career"}],"readingTime":3.305,"hasTruncateMarker":true,"authors":[{"name":"Denis","title":"Software Experience Dude","key":"denis"}],"frontMatter":{"slug":"why-a-blog","title":"Why this page exists","authors":["denis"],"tags":["career"]},"unlisted":false,"prevItem":{"title":"250mb json in a 40mb service limit","permalink":"/mynameis/blog/big-json"}},"content":"Blog is a\xa0 system to spread ideas across the internet. Shout out about a thing \\"Look, I have an opinion on that if you care\\".\\n\\n\x3c!--truncate--\x3e\\n\\nI have opinion on different things, especially in software. I\'ve been working on it since 2015.  \\n\\nSo let\'s start from reasoning why you might want to own one.\\n\\n## It makes more than 0 chances somebody will find it and share your opinion\\n\\nOr disagree, it\'s also fine and in some cases means almost the the same for you.\\n\\nYou get audience.\xa0\\n\\nAnd it\'s a big deal.\\n\\nFirst, the audience challenges your ideas. It\'s good to find a person to discuss your ideas. It allows your either find weaknesses in the idea itself, or perhaps your mindset to find crucial vocabulary to express what it means for you, or become even more confident on a topic. After all, it\'s just a joy to discuss what you have on top of your head.\\n\\n## More than 0 chances your future team knows you\\n\\nIt\'s a well known that having a referral in a company makes hiring easier to everyone. They get plus a point in confidence they hire not a dumbass, you may skip long questionnaire. They refer you not because you are good at contribution, but it\'s easier to build ubiquitous language in the team when you know each other.\\n\\nI want to say having written something valuable for others makes you a little more known that you used to be. And if you apply to a new job and they have found it - it bumps your chances a lot.\\n\\nIm convinced in the idea that not just delivering good result to your company, but telling the best pieces to the others will pay me off later on.\\n\\nThe only thing is under question if we really have built and understood something important for the industry.\\n\\n## Memo is good\\n\\nDuring software experience I realised the wide range technologies I apply can hardly be stored in my head.\\n\\nTools such Obsidian are fancy, but Im not disciplined enough to handle my cloud to store it or just structure them well enough, it doesn\'t fit me.\\n\\nSo Im going to structure my experience here.\\n\\nIt\'s not just putting all I have in my memory, but also try structuring all new stuff I have in my head.\\n\\nFor instance, I want to memoize a way to define alerts using terraform. Even though I\'ve done it I have no idea what those queries to an observability provider mean.\xa0\\n\\nWhat should I do?\\n\\nNot to fool people. I will figure out in order to explain it to the others. Otherwise the article will be one of the thousands on medium or linkedin.\\n\\nEventually I have to push my skill up to give my audience valuable experience shared.\\n\\nIn the same time I get structured my experience and making it better by reading more in order to write a little.\\n\\n## Expressing better\\n\\nAnd my personal reason. \\n\\nIm poor at structuring my speach. If you read it you know how hard it was to catch my ideas. But I fight against it and I believe the blog may make me write more often and amend the way I express myself.\\n\\n## You are ready. what now?\\n\\nOk, let\'s say you confirmed you have a reason to have a blog.\\n\\n  \\n\\nA couple of hints I want to add.\\n\\n  \\n\\nPlatforms like medium blocks people from reading your content without subscription. \\nAnd definitely it\'s not what I want.\\n\\nI can understand if you want to setup patreon to provide sub only data, but I don\'t consider such option.\xa0\\nMoreover, the platforms get bigger revenue chunk of your ad and don\'t do more than google does.\\n\\nI picked docusaurus because I have full control on codebase, fonts, styles, ads, comments, structuring. And I was lucky enough to find a template I wanted to reuse by [takken](https://takken.io/).\\n\\nOr you can just clone this [project](https://github.com/dennypenta/mynameis)."}]}')}}]);