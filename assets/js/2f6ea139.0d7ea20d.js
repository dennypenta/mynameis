"use strict";(self.webpackChunkmynameis=self.webpackChunkmynameis||[]).push([[538],{6362:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"web-sucks","metadata":{"permalink":"/mynameis/blog/web-sucks","source":"@site/blog/11-web-sucks/index.mdx","title":"Web in 2026 is a tragedy","description":"Context","date":"2026-01-06T22:22:27.000Z","formattedDate":"January 6, 2026","tags":[{"label":"software","permalink":"/mynameis/blog/tags/software"}],"readingTime":6.485,"hasTruncateMarker":true,"authors":[{"name":"Denis","title":"Software Experience Dude","key":"denis"}],"frontMatter":{"slug":"web-sucks","title":"Web in 2026 is a tragedy","authors":["denis"],"tags":["software"]},"unlisted":false,"nextItem":{"title":"PaaS devlog |#1","permalink":"/mynameis/blog/tq-devlog-1"}},"content":"## Context\\n\\nI do software for 10 years.\\nAt the beginning I did pure web: django, backbone, jquery, gulp, react, you know.\\n\\nSince then the rest of my career I did mostly Go backends: some data processing from elastic to postgres, from kafka to mongo, kubernetes integrations, terraform providers, javascript sandboxes and just simple RESTful APIs.\\nI have a few friends from web, mobile dev and I tried developing a desktop app, so to me it feels easy to imagine what to expect developing a declarative UI.\\n\\n\x3c!--truncate--\x3e\\n\\n### What do I want to see in a development flow?\\n\\n1. Declarative UI. I don\'t mind seeing XML like things in desktop or mobile, so I anticipate having old fashion HTML with a few tricks is ok.\\n2. Strict schema. In a designated API service ideally we have OpenAPI. But we do web, right? I see 2 options:\\n   - we have the schema first and generate code for both a client with options to inject interceptors, for instance to inject an auth token from the cookies (pls don\'t use local store for auth tokens). Protobuf provides this kind of experience, I can\'t get why it\'s not deeply in the culture to have same standard with OpenAPI, or Swagger if you will\\n   - or we have a server that defines client\'s available methods to call and its data model\\n3. Predictability. We all need debug the apps sooner or later. And if you are for a while in the industry you want to go deeper to the function definition to read its source in order to understand the problem. It makes reading documentation a bit easier and learning process smoother. \\"No magic\\" term is fine for me.\\n4. Clear separation of concerns. Back then in django/rails/etc. We had a view layer and we knew it\'s the only layer might be exposed to the user. I wouldn\'t say it was easy to understand how to expose sensitive data to the client, but it was simple.\\n\\n### So how was it 10 years ago?\\n\\nSuboptimal.\\nPeople were looking for decent approaches to build web.\\n\\nSince then we have amazing changes in web:\\n\\n1. tailwind: it\'s not only about dev experience, but a huge jump to reduce client bundle size dramatically. It\'s even more beneficial for dummies like me, having such a simple tool to produce efficient UIs, it feels amazing.\\n2. Tons of headless UI libraries and shadcn, it gave every small team the ability to establish its design system in 20 minutes, huge step forward.\\n3. Vite is a standard for building apps. It has so many smart moves in there increasing quality for anyone who steps into it, while webpack made manually come up with things to minimize the bundle.\\n4. React and friends. For anyone touching it it\'s clear - it makes sense. Having HTML right in the view layer shows you the markup of the component and its state. After things like dojo and backbone - ofc it\'s a way better.\\n5. Astro. Not well adopted yet, but static content also jumped forward. Behind vite config it hides dead simply but efficient idea - islands. Im pretty sure it gotta change SSR and SSG forever.\\n\\nWell, lots of positive things, let\'s build an app after a long break.\\n\\n## Picking UI\\n\\nReact. Well, it\'s changed a lot.\\nI don\'t want to be an old farter who complains on changes, but there are tons of features people rarely use.\\nOk, it\'s a matter of taste.\\nBut it made the bundle size jumping ridiculously high, it\'s not ok.\\nTreeshaking you tell me? Doesn\'t seem it helps much, it\'s still the most bloated UI library.\\nWe thought progressive internet quality and speed around the globe, but we make software worse to make the progress diminishing.\\n\\nVue has a bunch of magic where a \\"property\\" is not case sensitive may break the entire app, seems odd to me.\\n\\nThe only 2 real alive alternatives are Svelte and Solid.\\nSvelte is clearly taking a track to become the modern rails and Im not very fond of it.\\n\\n## Giving it API\\n\\nThere are many options today:\\n\\n- Write a regular api service, generate OpenAPI, generate models, etc.\\n- Use trpc/orpc like and generate API from that. To me it feels correct, we have a strict schema communication, breaking on one side we have a type error on the other. Then generating a schema from the implementation we document what we implemented instead of promising what we plan to implement. This approach eliminates overpromising and that\'s a big deal. We can see its evolution in svelte remote functions and similar attempts in next/nuxt/solid ecosystems.\\n\\nAdding Next JS to React apparently make it even larger in bundle size, so not every meta framework is a good choice.\\nBut not having a meta framework makes SSR a way harder.\\n\\nThere are alternatives to solve SSR with traditional backends like inertia.js, but having different languages don\'t give so much joy of immediate data types sync between server and client apps. After touching trpc/orpc it feels wrong.\\nIf I need a niche purpose backends like Go for integration with networking services chances I need SSR close to 0, so I can generate openapi from it and then client models with 0 worries.\\n\\n## Building all together\\n\\nWhat do we have left if I want:\\n\\n- small bundle size\\n- decent performance\\n- well defined unified API layer\\n- no magic solution\\n- SSR\\n\\nI have nothing.\\n\\nTraditional backends + inertial.js may save us, but introduce tons of others complexities, so in the best world we look the answer in the meta frameworks.\\n\\nReact is no go with a raw bundle size of ~100kb today, adding any UI library makes it at 150, adding a bit of code makes it 300+ in the best case.\\n\\nVue might be very close, but having case insensitive attributes drive me nuts. things like `v-for=\\"item in items\\"` undebuggable. how do I know the items there is what I meant? no way, only to run the app and take a look or pray my team writes the tests. A single typo may destroy a page.\\n\\nSvelte is promising in quality: small bundle size, great performance.\\nBut so much magic frustrates me.\\nLike in rails there is no logic, everything is declarations of the concepts.\\nI have to learn the framework instead of software fundamentals to understand $state, +page is not just a file name and a remote function is the actual API call depending on the execution context (server or client).\\nIt doesn\'t give \\"simple\\" solution, but gifts the \\"simplicity\\" with the price of an abstraction layer worse than ever before.\\nAnd all of it with the mixed context where is the server where is the client makes it even worse.\\n\\nSolid has all the UI benefits of React, all the performance benefits of svelte.\\nBut doesn\'t have any adoption. Its UI libraries are months passed since the last commit.\\nSolid itself and Solid Start have 3 weeks gap since the last commit.\\nIm more than confident it\'s simply due to lack of financial support and the need of having a commercial job for the maintainers.\\n\\n## Conclusion\\n\\nCloser to finish I find this 30 minutes writing as a pure nagging about sveltekit having too much magic and solid not enough adoption. Although most of the people don\'t share same feeling.\\n\\nIt\'s surprisingly (no, it\'s not) sad to me people had better learning frameworks than software itself.\\n\\nPeople don\'t like simplicity and it frustrates me as very few people look for it.\\n\\nAccept the rules, take whatever worse we have I guess and keep going if you need web.\\nI would be happy to see Solid having independent financing and larger community engaging."},{"id":"tq-devlog-1","metadata":{"permalink":"/mynameis/blog/tq-devlog-1","source":"@site/blog/07-tq-devlog-1/index.md","title":"PaaS devlog |#1","description":"Devlog #1: github apps, docker builder, cdk8s","date":"2026-01-01T18:27:10.000Z","formattedDate":"January 1, 2026","tags":[{"label":"paas","permalink":"/mynameis/blog/tags/paas"}],"readingTime":4.325,"hasTruncateMarker":true,"authors":[{"name":"Denis","title":"Software Experience Dude","key":"denis"}],"frontMatter":{"slug":"tq-devlog-1","title":"PaaS devlog |#1","authors":["denis"],"tags":["paas"]},"unlisted":false,"prevItem":{"title":"Web in 2026 is a tragedy","permalink":"/mynameis/blog/web-sucks"},"nextItem":{"title":"How Treenq builds workload images","permalink":"/mynameis/blog/container-in-container-09"}},"content":"## Devlog #1: github apps, docker builder, cdk8s\\n\\nToday I want to share with you my next steps of creating PaaS from scratch.\\nOn this page I will cover how I implement a basic deployment flow.\\n\\n\x3c!--truncate--\x3e\\n\\nThe service has the first api call. It\'s a github webhook, when a merge to main/master comes I spin a container from it.\\n\\n### Github apps\\n\\nIn order to listen users\' repository changes it\'s not necessary to use oauth anymore.\\nI can create a [github app](https://docs.github.com/en/apps/overview), kinda register an app and let my users to install it.\\nThis app will let me listen the events the users accepted.\\n\\nTo make the github webhook secure they provide sha in the headers so I implemented a [verifier](https://github.com/treenq/treenq/blob/main/pkg/crypto/signature.go).\\n\\n### App definition\\n\\nUsually users configure an app using a yaml or click buttons.\\nWe don\'t consider terraform for now, it\'s also viable, but we want a quick solution. \\n\\nI do plan implement a yaml, but now I want to focus on definition as code. I believe it\'s gonna give me more flexibility and give the users faster access to the app resources such as a database credentials.\\n\\nSo I expected a `tq` module in a go app like this:\\n```go\\npackage tq\\n\\nimport (\\n\\ttqsdk \\"github.com/treenq/treenq/pkg/sdk\\"\\n)\\n\\nfunc Build() (tqsdk.Space, error) {\\n\\treturn tqsdk.Space{\\n\\t\\tKey:    \\"treenq-poc\\",\\n\\t\\tRegion: \\"fra1\\",\\n\\t\\tService: tqsdk.Service{\\n\\t\\t\\tDockerfilePath: \\"Dockerfile\\",\\n\\t\\t\\tSizeSlug:       \\"basic-xxs\\",\\n\\t\\t\\tName:           \\"treenq-poc-service\\",\\n\\t\\t\\tHttpPort:       8000,\\n\\t\\t\\tInstanceCount:  1,\\n\\t\\t},\\n\\t}, nil\\n}\\n```\\n\\nEventually I want to come up with an idea how a user can map this model to the resources, but now I just want to retrieve the config a user defined such a Dockerfile path and the http port to expose.\\n\\nSo I did a builder package to generate a temporary folder, places this config in there, calls that `Build` func and reads the given config as a json output.\\n\\n### Build an image\\n\\nI found an official repo for [buildx](https://github.com/docker/buildx), it\'s in Go so I can embed it.\\nBut it looks quite a big challenge to solve, it\'s even scary to imagine the investigation to start using buildx, I would need to:\\n- run buildx command locally with a buildx debugger\\n- catch the exact function that builds an image\\n- understand all the dependencies and the way to build it\\n- put it into my service and make it work\\n\\nSo now Im fine just call `docker build` from CLI.\\nI definitely want to get back to buildx, but no clue when.\\n\\nAfter tagging the image I need to push it, so I added registry to my docker compose.\\nIt couldn\'t be simpler:\\n```yaml\\nregistry:\\n\\timage: registry:2.8.3\\n\\tports:\\n\\t\\t- \\"5000:5000\\"\\n```\\n\\n### Deploy\\n\\nNow I can deploy the image. This part is tricky. I couldn\'t really understand what Im gonna do here.\\n\\nSo I found [cdk8s](https://cdk8s.io/).\\nInitially I defined a testing definition. \\nSeems work, even can return me a yaml output.\\nUnfortunately, it brings node runtime under the hood, but it pays off now.\\n\\nSo Im taking the `tq` app definition I showed earlier and create a k8s [definition](https://github.com/treenq/treenq/blob/6919be57726109ff3f37ce484db3d2457b4cb01e/src/services/cdk/kube.go#L38) from it.\\nI do a quite simple deployment, a service and an ingress, and return the generated yaml.\\n\\nNow I have to apply this yaml to the cluster.\\n\\n__Cluster?__\\n\\nOk, I have to install a cluster.\\n\\n### Local kube\\n\\nThere are plenty of options to choose, but I start caring about the resources I use.\\nMy app started with postrgres, then registry, now it has a kube. The main issue I might be able to run a cluster in CI for integration tests, so I want to make my kube instance as small as possible.\\n\\nI found out k3s is the smallest distrubtion. Even though it doesn\'t follow all the standards, strangers in the internet convinced me I shouldn\'t ever understand the difference.\\n\\nSo I found an interesting [guide](https://sachua.github.io/post/Lightweight%20Kubernetes%20Using%20Docker%20Compose.html) how to start a cluster locally and it worked well.\\n\\nIn the progress I needed to add some kubelet [arguments](https://github.com/treenq/treenq/blob/6919be57726109ff3f37ce484db3d2457b4cb01e/docker-compose.yaml#L28) and the flight is done.\\n\\n### Deploy\\n\\nI wish I could use CLI and call `kubectl apply`, but it\'s not gonna workout.\\nThe goal to support a huge amount of clusters, so I accept kubeconfig dynamically as an argument.\\n\\nThe examples to make it I found only using regular rest api that kube provides. \\nBut I don\'t need it, but definitions I have are in yaml. \\n\\nSo I found some dynamic client to create resources unsafely.\\nSince I expected to not now what resources I create/update it what I need.\\nYes, still unsafe, but Im here.\\n\\nEventually I combine apimachinery and go client together to split the yaml definition by objects, unmarshal them and try creating or updating an object one by one.\\n\\nLater on I want to understand better what I need to create or update, but I would need to implement infra diffs for it, it\'s not gonna happen soon.\\n\\n### Test\\n\\nOn the testing step I discover my cluster can\'t pull an image since registry is not a TLS server.\\nIt\'s solved proving a registry config to my cluster.\\nSo I add another [volume](https://github.com/treenq/treenq/blob/6919be57726109ff3f37ce484db3d2457b4cb01e/docker-compose.yaml#L49) to my cluster to add the registries config\\n```yaml\\nmirrors:\\n  \\"registry:5000\\":\\n    endpoint:\\n      - \\"http://registry:5000\\"\\n\\nconfigs:\\n  \\"registry:5000\\":\\n    tls:\\n      insecure_skip_verify: true\\n```"},{"id":"container-in-container-09","metadata":{"permalink":"/mynameis/blog/container-in-container-09","source":"@site/blog/10-container-in-container/index.mdx","title":"How Treenq builds workload images","description":"Build a container inside a rootless conatiner","date":"2025-06-01T21:59:56.000Z","formattedDate":"June 1, 2025","tags":[{"label":"software","permalink":"/mynameis/blog/tags/software"},{"label":"paas","permalink":"/mynameis/blog/tags/paas"}],"readingTime":8.055,"hasTruncateMarker":true,"authors":[{"name":"Denis","title":"Software Experience Dude","key":"denis"}],"frontMatter":{"slug":"container-in-container-09","title":"How Treenq builds workload images","authors":["denis"],"tags":["software","paas"]},"unlisted":false,"prevItem":{"title":"PaaS devlog |#1","permalink":"/mynameis/blog/tq-devlog-1"},"nextItem":{"title":"gopls: how your IDE becomes better every day","permalink":"/mynameis/blog/gopls-08"}},"content":"## Build a container inside a rootless conatiner \\n\\nA PaaS has a very dev friendly use case: when a user just conneects a repository we must build the image from a Dockerfile in the repo. \\nWe assume at the beginning a team doesn\'t have a CI pipeline and wants to share a quick demo to the customers.\\nThere are many ways to do it. Let\'s look at all of them and at the design we chosed for [Treenq](https://github.com/treenq/treenq)\\n\\n\x3c!--truncate--\x3e\\n\\n### Grug: docker in docker \\n\\nAs a straight forward approach we can forward docker host to a container and build it.\\nIt has quite a lot of problems:\\n- dind requirees running in privileged mode: it opens a rabbit hole to a host machine\\n- moreover, it doesn\'t seem an option to make it rootless \\n- docker daemon is running, it\'s not free for resources \\n\\nEven to get started the user case it didn\'t seem convinient, it requires configuration to make it possible, so I decided to move to more secure options.\\n\\n### Kaniko: k8s native builder \\n\\nKaniko is created to solve this problem. \\nIt allows setting up an environemnt for a cluster where the workload is built near it\'s runners, perhaps on different nodes in order to avoid any resource fight.\\nThe idea is it spans a new container that builds an image, the container feels great in unprivileged environment and doesn\'t require a docker daemon.\\n\\nTechnically it doesn\'t create a new namespace, instead it unpacks an image layer and does chroot. \\n\\nYou can even span your registry right in this cluster, like a regular one or [Harbor](https://github.com/goharbor/harbor) and the builds flow is completely isolated from the planet.\\nIt like like __kaniko -> registry -> k8s workload node__.\\n\\nIn terms of security Kaniko is the winner. \\nIt does require root to run, but the abstraction is well done to keep it away. \\n\\nLooks fancy, but has a couple restrictions:\\n- it buidls containers only inside a Kubernetes cluster, it works in k8s, k3s (which we use for e2e tests) and perhaps some more.\\n- it has no more capabilities, if I need to inspect an image. \\n- it\'s not able to build windows images \\n- build time performance is slightly lower \\n\\nFor instance, I want to rollback, I want it real quick, so before launching a build job I want to look if the image is in the registry.\\nFor that purpose I must use skopeo or another tool to talk to a registry.\\nNot really a big deal.\\nPerhaps, one day Treenq will need [Skopeo](https://github.com/containers/skopeo).\\n\\nLooking back it looks a good option and most likely I would say it\'s worth trying it. \\nBut I thought it\'s very important to have a capability to run builds outside of a cluster, may be even on a isoalted VM, which is not really smart requirement I guess.\\nAlso, the complexity to set it up didn\'t look easy enough, I have to span a k8s resource, make a ServiceAccount for it and many more to start building + adding inspection API near by.\\n\\nOn top of that, recently, Kaniko stepped down right into arhive.\\n\\n### Buildah: isolated package\\n\\n[Podman](https://github.com/containers/podman), a Docker alternative, has a way cleaner opensource relation and how they structure the packages and repositories.\\nPodman has a designated module, [Buildah](https://github.com/containers/buildah), which is used exactly for building images.\\nIt requires no priovileges, root, daemon, nothing (technically speaking, but there is a price for it).\\n\\nIt doesn\'t require a daemon to run.\\nPackage is written in Go, therefore I can embed it right into the service instead of adding a syscall.\\n\\nDISCLAIMER: I understand I use undocumented API of the package and it can hurt it. If it happens I can freeze the version, and then either the project fulfills the graveyard or it migrates to buildkit.\\n\\nFirst step is to understand how to run buidlah in a container. \\nThere is an official [image](https://hub.docker.com/r/buildah/buildah) based on Fedora, but I want my app run at least in Ubuntu.\\nBetter in alpine/scratch ofc, but it doesn\'t seem viable.\\n\\nThere are a some guides on [how to run buildah in a container](https://developers.redhat.com/blog/2019/08/14/best-practices-for-running-buildah-in-a-container) and [in privileged environment](https://opensource.com/article/19/3/tips-tricks-rootless-buildah), so I can slowly transform this information to prepare Ubuntu image.\\nHere is what I got [eventually](https://github.com/treenq/treenq/blob/main/Dockerfile).\\n\\nOk, we can build, next step I want to build a small image from Go application.\\n\\nThere is a [guilde](https://github.com/containers/buildah/blob/main/docs/tutorials/04-include-in-your-build-tool.md) showing how programmatically execute dockerfile steps (run, copy, etc.), but it\'s not what we need.\\nWe simply need to run `buildah build`.\\n\\nAt this point I have to dig the source code to understand what\'s happening.\\nJumping right inside the build command in the source code we find there is a storage resource is required to start and we can execute the build \\n```go\\n\\tbuildStoreOptions, _ := storage.DefaultStoreOptions()\\n\\tbuildStore, _ := storage.GetStore(buildStoreOptions)\\n```\\n\\nAnd next step we can start build:\\n```go \\n\\tid, _, err := imagebuildah.BuildDockerfiles(ctx, buildStore, define.BuildOptions{\\n\\t\\tContextDirectory: args.Path,\\n\\t\\tRegistry:         a.registry,\\n\\t\\tOutput:           args.Name,\\n\\t\\tOut:              out,\\n\\t\\tErr:              errOut,\\n\\t\\tReportWriter:     reportOut,\\n\\t\\tIgnoreFile:     dockerignorePath,\\n\\t\\tAdditionalTags: []string{image.FullPath()},\\n\\t}, args.Dockerfile)\\n```\\n\\nTo push the image we need to get a reference to this image in our store and prepare system context: registry authentication, tls flags, certs.\\n```go \\n\\tstoreRef, _ := alltransports.ParseImageName(\\"docker://\\" + image.FullPath())\\n\\tsystemContext, := &types.SystemContext{\\n\\t\\tDockerCertPath:              a.registryCertDir,\\n\\t\\tDockerInsecureSkipTLSVerify: types.NewOptionalBool(!a.registryTLSVerify),\\n\\t\\tDockerAuthConfig:            &types.DockerAuthConfig{\\n\\t\\t\\tIdentityToken: a.registryToken,\\n\\t\\t},\\n\\t} \\n\\t_, _, err = buildah.Push(ctx, id, storeRef, buildah.PushOptions{\\n\\t\\tStore:         a.store,\\n\\t\\tSystemContext: systemContext,\\n\\t})\\n```\\n\\nFull version of this module is [here](https://github.com/treenq/treenq/blob/main/src/repo/artifacts/docker.go).\\n\\nOne important detail: first lines of your main function must be:\\n```go\\n\\tif buildah.InitReexec() {\\n\\t\\treturn\\n\\t}\\n\\tunshare.MaybeReexecUsingUserNamespace(false)\\n```\\n\\nIt matters to act Buildah as root without being real rool. \\nIt locks the OS thread, prepares uid/gid mapping and reexec as a new user in a new namespace. \\nSo, softly speaking, it\'s not ideal to run this code inside the application code.\\n\\nThe best design would be to have the application code isolated from pool of builders. Builders can receive the build tasks and still execute it inside a Go app in order to being able to extend he capabilities.\\nThis design allows me to extend options to build images: In a similar manner I can embed buildpacks to build simpler apps without a Dockerfile.\\n\\nBuildah, unfortunately, also has a few caveats and it\'s a little behined of docker.\\n- For instance buildah can\'t inject variables/secrets [on RUN command](https://github.com/containers/buildah/issues/5892) while docker can.\\n- Or it can\'t detect [variables in images](https://github.com/containers/buildah/issues/5408)\\n- Also, runniong it unprivileged is quite tricky and it requires specific arguments for container and specific build of the image itself. \\nThe domain is huge to learn, different backends, storages drivers and so on.\\n\\nFrom this [issue](https://github.com/containers/buildah/issues/2554) I learned vfs storage driver copies the entire sublayer on every layer.\\nSo it\'s better to use overlayfs, it requires /dev/fuse to run in non privileged container.\\n\\nAs a bonus, it might be cool to put it inside alpine, but not sure it\'s viable at all. \\nPutting a cgo app itself requires some [dance](https://baicai.xlog.app/docker_golang_alpine_cgo_build?locale=en) for it.\\n\\n### BuildKit: overlooked option\\n\\nWhat if docker could get rid of priviileged mode? \\n\\nBuildx (BuildKit) is a module of docker (or moby?) that\'s responsible specifically for building iamges.\\n\\nBuildKit still has a daemon and it requires a privileged mode either.\\n\\nHowever, we can run fake root using [RootlessKit](https://github.com/rootless-containers/rootlesskit/).\\nWe can build our flow in the following way:\\n- run a daemon \\n- run buildctl to start build \\n- when we are done we close the daemon \\n\\nRootlessKit requires turning off apparmor and seccomp, because the default profile doesn\'t allow spawning new namespaces.\\nAlso, add a few linux capabilities like SETUID, SETGUID and fake root users removing the option no-new-privileges.\\n\\nIt seems quite secure, but has a few constraints:\\n- requires support explicit uid and gid which is not fine and might be annoyign \\n- requires supporting a designated pool of build machines, which is actually makes sence in terms of designing a PaaS, but makes it a way complicated to start\\n\\n\\nIn case of buildah it required me to run the build in the same application.\\nIn order to scale it well I have to build network transport and mount volumes between them, which makes it quite complicated to setup.\\n\\nBuildkit provides its capabilities out of the box. It makes its API a bit  harder to learn, since it\'s not documented well and perhaps not supposed to be used like that.\\n\\nBuildkit brings to main modules to the table:\\n- buildkitd - a daemon to serve the builds, it may expose a unix socker to tcp connection\\n- buildctl - the actual buildkit client\\n\\nNow it requires to to setup additional container buildkitd and write a client part inside my Go service.\\nAs a benefit I get rid of building images and buildkit dependencies.\\n\\n```yaml\\n  buildkit:\\n    image: moby/buildkit:v0.23.0\\n    privileged: true\\n    container_name: buildkit\\n    ports:\\n      - \\"1234:1234\\"\\n    volumes:\\n      - ./buildkit/entrypoint.sh:/entrypoint.sh\\n      - ./buildkit/buildkitd.toml:/etc/buildkit/buildkitd.toml\\n      - ./registry/certs:/certs\\n      - ./buildkit/certs:/buildkit\\n    entrypoint: [\\"/entrypoint.sh\\"]\\n    healthcheck:\\n      test: [\\"CMD-SHELL\\", \\"buildctl --addr tcp://localhost:1234 debug workers\\"]\\n      interval: 5s\\n      timeout: 30s\\n      retries: 3\\n    extra_hosts:\\n      - \\"localhost:host-gateway\\"\\n```\\n\\nentrypoint specifies the arguments to run, it overrides the default address to tcp. \\nbuildkitd mostly contains registry and service access and its TLS to test and support self signed certs flow. \\nBuildkit not only builds images, but also pushes them, therefore it requires access to the registry. Since my service has acces to the local registry as to localhost I want my buildkit access it on localhost too, therefore I define extra_hosts there.\\nWithout the extra_hosts config registry will be available on registry host as define in the rest of the docker-compose.yaml.\\n\\nFull example is here: https://github.com/treenq/treenq/blob/main/src/repo/artifacts/docker.go\\n\\ndocker-compose is in the root of the repo.\\n\\nFinal design of the platform is the following:\\n![img](./design.svg)\\n\\nHappy to answer your questions below in the comments"},{"id":"gopls-08","metadata":{"permalink":"/mynameis/blog/gopls-08","source":"@site/blog/08-gopls/index.mdx","title":"gopls: how your IDE becomes better every day","description":"A few years ago we had 2 editors for Go: jetbrains plugin to IntelijIdea and YOURNAMEIT with a go plugin.","date":"2025-02-15T21:41:35.000Z","formattedDate":"February 15, 2025","tags":[{"label":"software","permalink":"/mynameis/blog/tags/software"},{"label":"go","permalink":"/mynameis/blog/tags/go"}],"readingTime":5.23,"hasTruncateMarker":true,"authors":[{"name":"Denis","title":"Software Experience Dude","key":"denis"}],"frontMatter":{"slug":"gopls-08","title":"gopls: how your IDE becomes better every day","authors":["denis"],"tags":["software","go"]},"unlisted":false,"prevItem":{"title":"How Treenq builds workload images","permalink":"/mynameis/blog/container-in-container-09"},"nextItem":{"title":"Go Dockerfile","permalink":"/mynameis/blog/go-dockerfile"}},"content":"A few years ago we had 2 editors for Go: jetbrains plugin to IntelijIdea and YOUR_NAME_IT with a go plugin.\\nFirst brought tons of closed source components that eventually became Goland and bunch of open source components used by another editor Go plugin:\\n\\nIt worked for a while, but it has a lot of issues.\\n\\n1. Number one, you install a plugin, it installs a tenish of the others, every version half of the tools are broken and that\'s very lame dance happened every release.\\n2. The integration complexity, every editor did its own integration set, some used only part of them, some integrated them one by one.\\n3. Performance, it\'s not obvious today, but every tool worked once per editor request, it means every click \\"Go to definition\\" it has index the code base, find the implementatons or the object locations navigate you.\\n\\n\x3c!--truncate--\x3e\\n\\n![tools](./tools.png)\\n\\n### LSP\\n\\nAnd Microsoft did a next step in creating IDE.\\nThey decided it\'s important to decouple Javascript language support from the the editor.\\nSo they created another component for autocompletion, code navigation, refactoring and diagnostic.\\nThis component must run outside of editor in order to let many vscode instances to reuse the same indexing and cache.\\nThe called it a language server and it made a new standard: LSP, Lanugage Server Protocol.\\n\\n[https://microsoft.github.io/language-server-protocol/specifications/lsp/3.17/specification/](https://microsoft.github.io/language-server-protocol/specifications/lsp/3.17/specification/)\\n\\n![gopls](./gopls.png)\\n\\nIt gives every language team to make a developer experience an editor independant delegating all the main editor capabilities to the LSP implementation.\\n\\nIt changes 3 fundamental things in the software history:\\n\\n- Language team supports any LSP compatible editor, it gives more editors come to the market like Helix, Zed, NVIM, etc.\\n- We finally don\'t need to install a bunch of binaries to make the development experience just \\"fine\\"\\n- And a bonus point, it gives a Go team to focus on supporting more complicated use cases like large repos support increasing performance for everyone, so it means everyone can make a dev experience better for everyone.\\n\\n### LSP Protocol\\n\\nLet\'s have a look at the protocol breifly.\\nIt has a client that drives all the necessary capabilities implemented on button clicks, hover, hotkeys, etc.\\nImplementing an LSP client it calls a server to resolve some actions.\\nFor instance clicking Go to definition it sends a request with a given editor position where the interface method is described.\\n\\n```json\\n{\\n  ...\\n  \\"method\\": \\"textDocument/definition\\",\\n  \\"textDocument\\": {\\"uri\\": \\"file://location/file.go\\"},\\n  \\"position\\": {\\"line\\": 42, \\"character: 3\\"},\\n  ...\\n}\\n```\\n\\nAnd the server contains all the indexed codebase can return a list of locations so editor suggest them to a developer.\\n\\n### gopls under the hood\\n\\nGlobally gopls has Cache.\\nOpening an editor it creates a new Session to connect to the server.\\nOpening a new project in the editor it creates a new Workspace looking for a module, go.work or GOPATH directory.\\nIt holds metadata to let gopls know where you actually want to navigate in the code.\\nIt uses default build, it also means it ignores build flags if you use them.\\n\\n:::info\\nIn case you wish to see better dev exprience:\\n\\n- changes to one module to be reflected in another\\n- optimize memory usage by reducing the number of builds it must track\\n- give gopls information where do you work\\n  then use go.work.\\n\\n  :::\\n\\n### Capabilities\\n\\nAll the features list you can find [here](https://github.com/golang/tools/blob/master/gopls/doc/features/README.md).\\nBelow I shortly describe what you could find useful to perform gopls at 100%.\\n\\n- General information and a doc string on hovering\\n- Diagnostics: static analysys, compile errors\\n- Completion: how vscode intellisense gives you suggestion or cmp/blink for nvim.\\n- Navigation: Go to definition, List references, List implementations, etc.\\n- Transformation: format, sort imports, and refactoring as rename, inline, extract, etc.\\n\\n<img src={require(\\"./goplsdemo.gif\\").default} />\\n\\n### What about Goland\\n\\nGoland is the opposite, it provides its capabilities as a closed source software. Lately it supports LSP as well, but they rarely can be compared in the feature set.\\n\\nThe main difference we all together can make gopls better, while we never can contribute to Goland.\\n\\nIf you found an idea like \\"I miss this hostkey to make this thing\\" or \\"generate that piece\\" or \\"discover that place\\" you probably want to develop another editor plugin.\\nIt happened it me either, but apparently it\'s not the best solution.\\nIf it\'s purely related to a language you just need to contribute to gopls.\\n\\n### How to debug gopls\\n\\nHere I describe a practical guide to gopls contribution.\\nI skip the issue discussion with your proposal, expected output, how you attach a visual picture and communicate nicely, gerrit user experience so on.\\n\\nAll the examples are using vscode, I\'m sure vim/emacs nerds have cracked it.\\nWe don\'t use GoLand here because it has a lot of internal tooling for Go that may add misunderstanding whether it\'s gopls or GoLand magic.\\n\\nFirst, let\'s configure a local debugging setup.\\nInside the repo we add a debugging statement for vscode in order to run our forked version of gopls, in a folder `tools/gopls/` we add:\\n\\n```json\\n{\\n  \\"version\\": \\"0.2.0\\",\\n  \\"configurations\\": [\\n    {\\n      \\"name\\": \\"Run Gopls\\",\\n      \\"type\\": \\"go\\",\\n      \\"request\\": \\"launch\\",\\n      \\"mode\\": \\"auto\\",\\n      \\"program\\": \\"./main.go\\",\\n      \\"args\\": [\\"-port=9000\\"]\\n    }\\n  ]\\n}\\n```\\n\\nNow you can run your gopls.\\n\\nIn another editor instance you add a workspace setting:\\n\\n```json\\n\\"go.languageServerFlags\\": [\\n  \\"-remote=localhost:9000\\"\\n]\\n```\\n\\nSo you know how to make a call from your editor to the server and found a place to debug.\\n\\nFor example you want to add a quickfix, it\'s a Code Action to make a Transformation, like implementing an interface or generating a function, etc.\\n\\nNow guess where to put a breakpoint and call a Code Action from a client, in vscode you need to push `CMD + .` or find your hotkey in the [doc](https://code.visualstudio.com/docs/editor/refactoring#_code-actions-quick-fixes-and-refactorings).\\n\\n### A few nice words in the end\\n\\nWhat\'s really important Go team uses LSP compatible editors, so real dog fooding comes in the room, they experience what the build and introduce the features they miss.\\nI know for sure Alan Donovan uses Emacs, at least he said so.\\n\\nIf you are still interested look his [speach](https://youtu.be/8EsaJC9cn4w) how they optimized Gopls last interation.\\n\\nA really hope it raises awareness across the entire Go community and lights the interest to the tooling, more people contribute better experience everyone would have."},{"id":"go-dockerfile","metadata":{"permalink":"/mynameis/blog/go-dockerfile","source":"@site/blog/07-go-dockerfile/index.mdx","title":"Go Dockerfile","description":"The things I\'ve collected to write my best Dockerfile. Appreciate any comments mentioning I could do it better and more optimal.","date":"2025-01-21T20:11:35.000Z","formattedDate":"January 21, 2025","tags":[{"label":"software","permalink":"/mynameis/blog/tags/software"},{"label":"go","permalink":"/mynameis/blog/tags/go"}],"readingTime":6.63,"hasTruncateMarker":true,"authors":[{"name":"Denis","title":"Software Experience Dude","key":"denis"}],"frontMatter":{"slug":"go-dockerfile","title":"Go Dockerfile","authors":["denis"],"tags":["software","go"]},"unlisted":false,"prevItem":{"title":"gopls: how your IDE becomes better every day","permalink":"/mynameis/blog/gopls-08"},"nextItem":{"title":"Unit test in observability","permalink":"/mynameis/blog/unit-test-observability"}},"content":"The things I\'ve collected to write my best Dockerfile. Appreciate any comments mentioning I could do it better and more optimal.\\n\\n\x3c!--truncate--\x3e\\n\\n## Preface\\n\\nTLDR:\\n\\n```docker\\nFROM golang:1.23.1-alpine AS builder\\n\\nWORKDIR /app\\n\\nENV CGO_ENABLED=0 \\nENV GOOS=linux\\n\\nCOPY go.mod go.mod\\nCOPY go.sum go.sum\\nRUN --mount=type=cache,target=/go/pkg/mod/ go mod download -x\\n\\nCOPY . .\\n\\nFROM builder AS dev\\n\\nRUN --mount=type=cache,target=/go/pkg/mod/ --mount=type=cache,target=\\"/root/.cache/go-build\\" go install github.com/go-delve/delve/cmd/dlv@v1.23.0\\n\\nRUN --mount=type=cache,target=/go/pkg/mod/ --mount=type=cache,target=\\"/root/.cache/go-build\\" go build -gcflags=all=\\"-N -l\\" -o server ./cmd/server\\n\\nCMD [\\"dlv\\", \\"--listen=:40000\\", \\"--continue\\", \\"--headless=true\\", \\"--api-version=2\\", \\"--accept-multiclient\\", \\"exec\\", \\"server\\"]\\n\\nFROM builder AS prod\\n\\nRUN --mount=type=cache,target=/go/pkg/mod/ --mount=type=cache,target=\\"/root/.cache/go-build\\" go build -ldflags \\"-s -w\\" -o server ./cmd/server\\n\\nFROM alpine:3.13\\n\\nRUN addgroup -g 1001 appgroup && adduser -D -G appgroup -u 1001 appuser\\n\\nWORKDIR /app\\n\\nUSER 1001\\n\\nCOPY --from=prod /app/server server\\n\\nCMD [\\"/app/server\\"]\\n```\\n\\nLink: https://github.com/treenq/treenq/blob/98e6d8dd5f5756fe5df561913e10515784ef7163/Dockerfile\\n\\nNow let\'s breakdown what\'s happening here and why.\\n\\n:::note \\nI use colima and all the things described here work well. However, all the docs referenses will go to docker. I think they did a great job to push the industry standart. Also you have to turn on buildx to make these features work. It requires buildx installation and setting a feature flag `DOCKER_BUILDKIT=1`.\\n:::\\n\\n## Base image\\n\\nFirst, we need a base image in order to build the app. We want the base being less as possible, for that purpose I use alpine:\\n\\n```docker\\nFROM golang:1.23.1-alpine AS builder\\n```\\n\\nYou also can consider [distroless](https://github.com/GoogleContainerTools/distroless), it\'s very suitable for interpreted languages like python/nodejs, but alpine works well for Go.\\n\\n## Dependencies\\n\\nThe next step we prepare a surface to build the image, all the necessary dependencies we install there.\\n\\nIf you need specific timezones, certificates, github private repo creds, the service modules, whatever, we do it right here.\\n\\n```docker\\nCOPY go.mod go.mod\\nCOPY go.sum go.sum\\nRUN --mount=type=cache,target=/go/pkg/mod/ go mod download -x\\n```\\n\\nWe must copy only desegnated dependencies definition and right after download them.\\nIf you don\'t know why and what is docker layers please refer to Docker basics and get back.\\n\\n`-x` Flag is kinda adds verbosity showing what `go mod download` executes.\\n\\nThe interesting details here is `--mount=type=cache`.\\nYou can find more in the [reference](https://docs.docker.com/reference/dockerfile/#run---mounttypecache)\\n\\nIf you update a dependency in your `go.mod` and rebuild it, then this statement will not download all the packages from scratch, it creates a designated mount and hold them in one place.\\n\\nUnfortunately, most of the CI systems create a new image for a new job and it doesn\'t work in CI, but helps a lot for local testing. For instance, I run e2e tests locally and it saves a couple of minutes every try.\\n\\nAnd only then we copy the rest of the codebase:\\n\\n```docker\\nCOPY . .\\n```\\n\\n:::note \\nDon\'t forget to check your dockerignore to skip copying useless files.\\n:::\\n\\n## Targets\\n\\nYou must have heard about multi-stage images.\\nAnd you have built the dockerfiles with 2 stages, first to build a binary and the next to run it in a blank environment.\\n\\nBut I will convince you to have 4 stages.\\n\\nDocker has an amazing feature: targets.\\n\\nThe targets allow to run a specified stage of the image.\\n\\nLet\'s have a look how it plays out in case of e2e tests.\\n\\nWe have a regular docker compose setup with a database or whatever dependencies you have to run your tests.\\nIm a big fan of a debugger and a big hater of infinite print statements. Don\'t get me wrong, logging is an awesome tool, but not the forgotten prints in a production build.\\n\\n```docker\\nFROM builder AS dev\\n\\nRUN --mount=type=cache,target=/go/pkg/mod/ --mount=type=cache,target=\\"/root/.cache/go-build\\" go install github.com/go-delve/delve/cmd/dlv@v1.23.0\\n\\nRUN --mount=type=cache,target=/go/pkg/mod/ --mount=type=cache,target=\\"/root/.cache/go-build\\" go build -gcflags=all=\\"-N -l\\" -o server ./cmd/server\\n\\nCMD [\\"dlv\\", \\"--listen=:40000\\", \\"--continue\\", \\"--headless=true\\", \\"--api-version=2\\", \\"--accept-multiclient\\", \\"exec\\", \\"server\\"]\\n```\\n\\nOn installing delve don\'t forget to specify cache mount, it has its own dependencies and it will help to speed the build up.\\nMoreover, if you don\'t mount the cache it won\'t discover the installeed dependencies in a previous step.\\n\\nThe mount flag might look unclear, as an argument you only put a path _where_ to mount, you can\'t control the source of directory, it\'s managed docker buildkit.\\n\\nThen we build a binary with a couple important gcflags, where `-l` disables inlining and `-N` removes optimisations. It matters because otherwise debugger won\'t be able to show some variables or navigate into some functions.\\nYou can read more [here](https://pkg.go.dev/cmd/compile).\\n\\nAs the last statement we run dlv.\\n\\nAnd that\'s what we have in a docker compose:\\n\\n```yaml\\nserver:\\n  build:\\n    context: .\\n    dockerfile: Dockerfile\\n    target: dev\\n  ports:\\n    - \'8000:8000\'\\n    - \'40000:40000\'\\n  security_opt:\\n    - seccomp:unconfined\\n  cap_add:\\n    - SYS_PTRACE\\n```\\n\\nHere we specify target field, clear and simple. You also can pass it as a cli argument to an image.\\n\\nPorts has 2 elements, a regular app HTTP port and a DAP (Debug Adapter Protocol) port that delve exposes.\\n\\nNext we add `secuty_opt` since the default Seccomp profile restricts the `ptrace` system call.\\n\\n:::note \\nSeccomp is a Linux kernel feature used to restrict the system calls that a process can make. By default, Docker applies a restrictive Seccomp profile to limit potentially dangerous system calls, improving container security. Read more [here](https://docs.docker.com/engine/security/seccomp/) and [here](https://docs.docker.com/engine/containers/run/).\\n:::\\n\\nWhen you specify `seccomp:unconfined`, it removes the Seccomp restrictions, allowing the container to make all system calls. This config allows running `ptrace` syscall in the container, delve uses it to set breakpoints, observe memory, etc.\\n\\nBut it\'s not enough. We have to not only remove a restriction, but explicitly give a permission, that\'s why we have `cap_add` statement: to add a capability for that syscall.\\n\\n## Build prod\\n\\nThe prod build is quite simple and well known:\\n\\n```docker\\nFROM builder AS prod\\n\\nRUN --mount=type=cache,target=/go/pkg/mod/ --mount=type=cache,target=\\"/root/.cache/go-build\\" go build -ldflags \\"-s -w\\" -o server ./cmd/server\\n```\\n\\nWe still use mount cache, we just put different build flags, in this case ldflags to achieve exactly the opposite we did in order to build a debug target.\\n`-s` and `-w` stand for skipping debug info, read more [here](https://pkg.go.dev/cmd/link).\\n\\n## Run prod\\n\\nThere is no lots of new things for you, I want to focus on a small important thing: a user.\\n\\n```docker\\nRUN addgroup -g 1001 appgroup && adduser -D -G appgroup -u 1001 appuser\\n\\nUSER 1001\\n```\\n\\nThere is so many information around on this security topic and I keep seeing zero attention to a user inside the image.\\n\\nShortly speaking - less ability less chance to make a mistake or open a vulnerability. Docker has a good blog [post](https://www.docker.com/blog/understanding-the-docker-user-instruction/) to cover why it matters.\\n\\n## Target dependenciy graph\\n\\nNow about the main concern of so many stages.\\n\\nWhy would I build all the stages for releasing my go app?\\n\\nYou will not, if you turned on buildkit it will behave as a smarter and build only the necessary dependencies, it means it builds the dependency graph and builds only necessary part, so your production CI will never install delve to waste your time.\\nThe [documentation](https://docs.docker.com/build/building/multi-stage/#differences-between-legacy-builder-and-buildkit) explains it very well.\\n\\n## A few caveats on debugging remote DAP.\\n\\nIf you start a debugging process as is you will find your breakpoints Rejected. \\nIt happens because your DAP communicates breakpoints state with DAP server using the client paths, and your client IDE is located on your machine, while the Go binary was built inside an image, another host machine.\\n\\nYou can find the fix in the official [doc](https://github.com/go-delve/delve/blob/master/Documentation/cli/substitutepath.md).\\n\\nFirst, I would connect to dlv `dlv connect localhost:40000` and test path substitution, \\nfor instance `config substitute-path /path/in/docker /local/path` where /path/in/docker is just your WORKDIR statement and /local/path is your local dir (input `pwd` in the project folder).\\nAfter that you can try `list main.main` and make sure it lists you a main function without an error.\\n\\nEventually I have the following config to configure remote debugger:\\n```json\\n{\\n      \\"type\\": \\"go\\",\\n      \\"name\\": \\"debug remote service\\",\\n      \\"mode\\": \\"remote\\",\\n      \\"request\\": \\"attach\\",\\n      \\"port\\": 40000,\\n      \\"substitutePath\\": [\\n        {\\n          \\"from\\": \\"${env:HOME}/projects/project-name\\",\\n          \\"to\\": \\"/app\\"\\n        },\\n        {\\n          \\"from\\": \\"${env:HOME}/go/pkg/mod/\\",\\n          \\"to\\": \\"/go/pkg/mod/\\"\\n        }\\n      ]\\n    }\\n```\\n\\n## Conclusion\\n\\nSorry, have nothing to say. Appreciate if you leave things better than you found it."},{"id":"unit-test-observability","metadata":{"permalink":"/mynameis/blog/unit-test-observability","source":"@site/blog/06-unit-test-observability/index.md","title":"Unit test in observability","description":"On this page, I want to cover the caveats I encountered while shaping my approach to unit testing in the context of observability.","date":"2024-10-06T21:43:35.000Z","formattedDate":"October 6, 2024","tags":[{"label":"software","permalink":"/mynameis/blog/tags/software"}],"readingTime":4.535,"hasTruncateMarker":true,"authors":[{"name":"Denis","title":"Software Experience Dude","key":"denis"}],"frontMatter":{"slug":"unit-test-observability","title":"Unit test in observability","authors":["denis"],"tags":["software"]},"unlisted":false,"prevItem":{"title":"Go Dockerfile","permalink":"/mynameis/blog/go-dockerfile"},"nextItem":{"title":"Web API Design: A Simplified Approach","permalink":"/mynameis/blog/api-design"}},"content":"On this page, I want to cover the caveats I encountered while shaping my approach to unit testing in the context of observability.\\n\\nIt\'s important to clarify that I\u2019m not referring to testing logs or metrics delivery. \\nInstead, I want to focus on testing modules that perform business logic but also include observability calls, like loggers or meters.\\n\\n\x3c!--truncate--\x3e\\n\\n## Regular Unit Test\\n\\nLet\u2019s start with a typical example of a unit test containing a logger call. \\n\\n```go\\nfunc (s *service) Do(ctx context.Context, request model.Object) (model.Response, error) {\\n\\tres, err := s.repo.Do(ctx, request)\\n\\tif err != nil {\\n\\t\\treturn res, err\\n\\t}\\n\\n\\treturn res, nil\\n}\\n```\\n\\nWith a straightforward approach, you\'d mock the repository and verify in a unit test that the mock was called with the expected parameters.\\n\\nThis is the conventional way of handling things.\\n\\n## Side Effects\\n\\nNow, let\u2019s complicate the example by adding a logger.\\n\\n```go\\nfunc (s *service) Do(ctx context.Context, request model.Object) (model.Response, error) {\\n  err := s.notImportantRepo.Something(ctx, another.Model(request))\\n  if err != nil {\\n    s.log.Error(\\"can\'t do something\\", \\"err\\", err.Error())\\n  }\\n\\n\\treturn nil\\n}\\n```\\n\\nFollowing the same approach, we can mock both dependencies. Here\'s how the mock preparation might look:\\n```go\\nnotImportantRepoErr := errors.New(\\"err\\")\\nnotImportantRepo.On(\\"Something\\", req).Return(notImportantRepoErr)\\nlog.On(\\"Error\\", mock.Anything, \\"err\\", notImportantRepoErr.Error())\\n```\\n\\nIf we make the logger call more realistic, extracting all the values we need for emergency debugging, we end up with this:\\n```go\\ns.log.Error(\\"can\'t do something\\", \\"err\\", err.Error(), \\"ctx\\", logging.FromContext(ctx), \\"another\\", mapper.From(req))\\n```\\n\\nAt this point, engineers often use mock.Anything for the logger parameters:\\n```go\\nlog.On(\\"Error\\", mock.Anything...)\\n```\\n\\nThis leads to a couple of issues:\\n- The logger call becomes a side effect that doesn\u2019t affect the result being tested.\\n- Nobody bothers to test what\'s actually logged.\\n\\n## Sustainability\\n\\nReal-world software isn\u2019t always as clean as in theory, raising a few important questions.\\n\\n1. __1.__ Does this approach improve the __quality__ of logging?\\n\\nIn some ways, yes. \\nAt least we confirm that a log message with the correct severity level was sent. \\nHowever, ensuring that the necessary parameters are logged requires strict discipline, which isn\u2019t always feasible. \\nWe can\u2019t \\"fix people.\\"\\n\\n2. __2.__ Does it __reduce PRs__ focused solely on improving logging?\\n\\nSadly, no. \\nThere\'s no validation of the parameters being logged, their mapping, formatting, or even the actual log message.\\n\\nOften, only in production do we realize, \\"Oh, it\'s missing field X.\\" After another debugging round, we might say, \\"Oh, it\'s missing field Z.\\" This cycle continues.\\n\\n3. __3.__ Does it help deliver more __observable software__?\\n\\nThis is tricky. \\nWhile it might seem like a yes (if we answer question 1), the lack of validation means we\u2019re still prone to errors. \\nThe absence of data model testing only slows down software delivery.\\n\\nEvery time an engineer touches the function, they might copy a mock from a previous test. \\nIf the function behaves differently, they\u2019ll see a failed unit test like this:\\n```\\nassert: mock: I don\'t know what to return because the method call was unexpected.\\n  Either do Mock.On(\\"Error\\").Return(...) first, or remove the Error() call.\\n  This method was unexpected:\\n    Error(params)\\n```\\n\\nThen, they\u2019ll blindly copy the previous mock statement, and the test will pass\u2014despite the logger parameters being completely different.\\n\\n## Is It Worth It?\\n\\nHere\u2019s my evaluation:\\n- Patching takes more time because of the added mock statements.\\n- More CI failures occur due to forgotten mocks.\\n\\nAll these costs offer no real benefit. \\nWe only confirm that the log was called. \\nBut for all we know, we might have sent nil to the logger and won\u2019t realize it until an emergency.\\n\\n## A Better Approach\\n\\nThere are two paths to improve this situation:\\n- Never allow loggers to accept mock.Anything.\\n- Don\u2019t test loggers in the business layer.\\n\\nThe first option is ideal but impractical unless you have years to experiment with uncertain payoffs.\\n\\nThe second option is more radical but effective: don\u2019t mix observability tests with business logic. \\nLet me explain in two steps.\\n\\n#### Nop Logger\\n\\nStart by defining the logger mock as a no-op and remove it from the mocks preparation. \\nYou can use one of the following loggers for unit tests:\\n\\n- An empty value logger\\n- A package-defined nop logger\\n- A mock logger with no assertions\\n- A logger that writes to io.Discard (no impact on unit test output)\\n\\nAny of these options will ignore logger calls and simplify development.\\n\\nYou can apply the same approach to other observability components. \\nFor example, _OpenTelemetry_ provides `\\"go.opentelemetry.io/otel/metric/noop\\"` to create an empty counter or meter.\\n\\n#### Isolation\\n\\nFinally, I\u2019m not suggesting we skip logging tests entirely.\\n\\nWe should still test them, but in __isolation__ rather than as a side effect. \\nHere\u2019s how:\\n\\n```go\\n\\tfor _, tt := range []testCase{\\n\\t\\t{\\n\\t\\t\\tname: \\"regular log\\",\\n\\t\\t\\thandler: http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\\n\\t\\t\\t\\tLoggerFromContext(r.Context()).ErrorContext(r.Context(), \\"test\\", \\"arg1\\", \\"text\\")\\n\\t\\t\\t\\tw.WriteHeader(200)\\n\\t\\t\\t}),\\n\\t\\t\\texpectedLogFunc: func(t *testing.T, m map[string]string) {\\n\\t\\t\\t\\tt.Helper()\\n\\t\\t\\t\\tassert.Equal(t, \\"ERROR\\", m[\\"level\\"])\\n        ...\\n\\t\\t\\t},\\n\\t\\t\\texpectedStatus: 200,\\n\\t\\t},\\n\\t} {\\n\\t\\tt.Run(tt.name, func(t *testing.T) {\\n\\t\\t\\tbuf := bytes.NewBuffer(nil)\\n\\t\\t\\tl := NewLogger(buf, slog.LevelInfo)\\n\\t\\t\\tm := NewLoggingMiddleware(l)\\n\\t\\t\\th := m(tt.handler)\\n\\n\\t\\t\\tw := httptest.NewRecorder()\\n\\t\\t\\th.ServeHTTP(w, httptest.NewRequest(\\"POST\\", \\"/\\", nil))\\n\\n\\t\\t\\tres := buf.String()\\n\\t\\t\\tjsonRes := make(map[string]string)\\n\\t\\t\\terr := json.Unmarshal([]byte(res), &jsonRes)\\n\\t\\t\\trequire.NoError(t, err)\\n\\n\\t\\t\\ttt.expectedLogFunc(t, jsonRes)\\n\\n\\t\\t\\tassert.Equal(t, tt.expectedStatus, w.Code)\\n\\t\\t})\\n\\t}\\n```\\n\\nInstead of writing to stderr/stdout, substitute the logger\u2019s writer with bytes.Buffer, allowing you to capture and validate the exact log message.\\n\\nThis way, we\u2019re testing the logger implementation itself, not the side effect of a function that has little to no impact on the application\'s core functionality."},{"id":"api-design","metadata":{"permalink":"/mynameis/blog/api-design","source":"@site/blog/05-api-design/index.mdx","title":"Web API Design: A Simplified Approach","description":"Note: This page won\u2019t teach you how to design an API from scratch, but it will give you insights into how I recently designed one while developing my PaaS.","date":"2024-09-19T20:45:05.000Z","formattedDate":"September 19, 2024","tags":[{"label":"software","permalink":"/mynameis/blog/tags/software"}],"readingTime":6.06,"hasTruncateMarker":true,"authors":[{"name":"Denis","title":"Software Experience Dude","key":"denis"}],"frontMatter":{"slug":"api-design","title":"Web API Design: A Simplified Approach","authors":["denis"],"tags":["software"]},"unlisted":false,"prevItem":{"title":"Unit test in observability","permalink":"/mynameis/blog/unit-test-observability"},"nextItem":{"title":"PaaS devlog |#0","permalink":"/mynameis/blog/tq-devlog-0"}},"content":"__Note__: This page won\u2019t teach you how to design an API from scratch, but it will give you insights into how I recently designed one while developing my [PaaS](https://github.com/treenq/treenq).\\n\\n\x3c!--truncate--\x3e\\n\\n## Preface\\n\\nFeel free to skip to the next section if you want to avoid the backstory.\\n\\nAfter working with various communication protocols (e.g., web form-data, REST, JSON-RPC 2.0), I\u2019ve realized that many of them aim to cover a wide range of use cases. This broad focus introduces a lot of complexity:\\n\\n- Network implementations require special handling, such as decoding domain-level errors into transport error codes and messages.\\n- Encoding and decoding messages require marshalling domain models into protocol-specific formats.\\n- Protocols like REST involve lengthy discussions about naming conventions and URL structures.\\n- Some protocols, like JSON-RPC, demand custom solutions, such as middleware for handling requests.\\n\\nThese factors complicate implementations and slow down team agreements.\\n\\n## The gRPC Shift\\n\\nWhen I started working with gRPC, it introduced some important improvements:\\n\\n- Models are generated automatically.\\n- Service interfaces are provided and generated.\\n- Clients are generated, allowing you to focus on just creating requests.\\n- Error codes are simplified, reducing them from hundreds to around 15 (which is still more than needed, but we\u2019ll get to that).\\n- The error format is predefined, so you don\u2019t have to design it\u2014just reuse [the model](https://cloud.google.com/apis/design/errors)\\n\\nAnd I liked. I don\'t know who didn\'t.\\n\\nI loved it. Most people who use gRPC feel the same.\\nHowever, gRPC doesn\u2019t naturally align with web browsers due to its reliance on HTTP/2, often necessitating additional layers to adapt to HTTP/1.1. \\nStill, browser compatibility remains a common requirement.\\n\\n## Bringing gRPC Ideas to the Web\\n\\nThis made me think: what can we adopt from gRPC for web APIs? The most obvious feature is __code generation__.\\n\\nSwagger attempts client generation but introduces its own challenges:\\n1. Swagger has too many plugins, requires specific formats, and its \\"standard\\" is fragmented. Many web renderers are incompatible with each other. For example, Apiary\u2019s requirements may contradict code generation.\\n2. Swagger focuses on defining documentation instead of the API itself, adding complexity.\\n\\nThe I discovered [tRPC](https://trpc.io/docs/quickstart), which offered many things I\u2019d been hoping for\u2014network handling as a natural consequence of defining models. \\nThe catch? It\u2019s limited to JavaScript.\\n\\nSo, I decided to design my own API approach.\\n\\n## My API Design Principles\\n\\nHere are the guiding principles I followed:\\n\\n- It must be built on top of HTTP to support the web.\\n- No URL queries, forms, or arguments in the path\u2014these overcomplicate things. Most people don\u2019t even know that URL queries are essentially [multimaps](https://en.wikipedia.org/wiki/Multimap), which leads to more bugs.\\n- Every API call uses only POST. The payload is always in the body, and for simplicity, in my case, it\u2019s JSON.\\n- The procedure naming follows the same conventions we use in code. For instance, instead of REST\u2019s `/users/{id}/wallet`, I use `/getUserWallet`.\\n\\n## What Does This Achieve?\\n\\n- I only define the models I need \u2014 no extra complexity.\\n- Optionally, I can add descriptions to make the generated documentation more talkative.\\n- From the defined router, I can generate both the client code and the API specification (including Swagger).\\n- The code generation process is simplified: a single - word URL, consistent HTTP method, and uniform marshalling strategy make everything easy to implement and maintain.\\n\\n## Error Handling\\n\\nError handling is a crucial part of any API design, and the model needs to be flexible enough to cover a wide range of scenarios. \\nBased on my experience, a simple yet effective error model looks like this:\\n\\n```go\\ntype Error struct {\\n  Code    string\\n  Message string\\n  Meta    map[string]any\\n}\\n```\\n\\n- __Code__: This holds a unique value that identifies the specific issue. \\nIt helps narrow down the location of the problem in the code and quickly leads developers to the exact line or area where the error occurred. Additionally, it provides clear guidance to users about the expected behavior or how to resolve the issue.\\n- __Message__: The message field can serve a dual purpose:\\n  1. __UserMessage__: This is a user-friendly message that can be translated into different languages on the server side. It\'s meant to be shown in the user interface to guide the user.\\n  2. __DevMessage__: This contains a technical explanation intended for developers or logs. It explains what went wrong, why it happened, and what actions can be taken to resolve it.\\nWhile you could use a single string for both, in complex applications with internationalization (i18n) needs, separating them into two distinct lines would help.\\n- __Meta__: This is a flexible map that can contain any additional context or metadata about the error. For example, if you\'re validating a form with multiple fields, the Meta map can list which fields failed validation and why.\\n\\n### Example Error Response\\n\\nHere\u2019s an example of how this error model might be used in a real-world scenario. Suppose a user submits a tax form with several invalid fields. The API can return an error response like this:\\n```go\\nError{\\n  Code: \\"TAX_FORM_INVALID\\",\\n  Meta: map[string]any{\\n    \\"tax_class\\": \\"expected a number between 1 and 4\\",\\n    \\"house_number\\": \\"must be a number\\",\\n  }\\n}\\n```\\n\\nIn this example:\\n- The __Code__ is `\\"TAX_FORM_INVALID\\"`, which indicates a validation failure specific to the tax form.\\n- The __Meta__ map provides detailed feedback on specific fields (`tax_class`, `house_number`) that need attention, making it easier for the user to correct their input.\\n\\n\\n### Error Handling in API Handlers\\n\\nIn my API, the handler interface is designed to consistently return errors using this model. \\nThis ensures that all errors adhere to a common structure, simplifying error handling on both the client and server side.\\n\\nFor instance, a typical API handler might look like this:\\n```go\\nGetProfile(ctx context.Context, _ struct{}) (GetProfileResponse, *Error)\\n```\\n\\nHere, instead of returning a plain error, the handler returns a pointer to the *Error type. \\nThis approach ensures that any error returned strictly follows the predefined error format.\\n\\n### Example from My Application\\n\\nHere\u2019s a practical example of how I\u2019ve implemented this in my app:\\n```go\\npackage domain\\n\\nimport (\\n    \\"context\\"\\n    \\"github.com/treenq/treenq/pkg/vel\\"\\n)\\n\\ntype GetProfileResponse struct {\\n    Email    string `json:\\"email\\"`\\n    Username string `json:\\"username\\"`\\n    Name     string `json:\\"name\\"`\\n}\\n\\nfunc (h *Handler) GetProfile(ctx context.Context, _ struct{}) (GetProfileResponse, *vel.Error) {\\n    profile := h.authProfiler.GetProfile(ctx)\\n    return GetProfileResponse{\\n        Email:    profile.Email,\\n        Username: profile.Username,\\n        Name:     profile.Name,\\n    }, nil\\n}\\n```\\n\\nIn this code:\\n- The `GetProfile` function fetches the user profile and returns it as a `GetProfileResponse`.\\n- The second return value is `*Error` instead of a generic `error`, making it clear that the function follows the structured error format.\\n\\n## Why Use a Custom Error Type?\\n\\nBy using a custom `Error` type instead of the standard `error`, we enforce consistency across the API. \\nThe error itself still implements Go\u2019s built-in `error` interface, allowing it to work seamlessly with existing Go error-handling patterns. \\nHowever, the benefit is that every error now carries additional structured information, like the error code and metadata, making it easier to debug and handle errors programmatically.\\n\\n## Epiloge\\n\\nI ended up building a \\"framework\\" in Go on top of net/http that is able to generate clients for any language.\\n\\nYou can check it out [here](https://github.com/treenq/treenq/blob/main/pkg/vel/router.go). \\n\\nI don\u2019t recommend using it as-is \u2014 it\u2019s better to copy and adapt it for your own needs."},{"id":"tq-devlog-0","metadata":{"permalink":"/mynameis/blog/tq-devlog-0","source":"@site/blog/04-tq-devlog-0/index.md","title":"PaaS devlog |#0","description":"Devlog #0: logging, linter, Zitadel, codegen, e2e tests","date":"2024-09-02T20:12:28.000Z","formattedDate":"September 2, 2024","tags":[{"label":"paas","permalink":"/mynameis/blog/tags/paas"}],"readingTime":8.43,"hasTruncateMarker":true,"authors":[{"name":"Denis","title":"Software Experience Dude","key":"denis"}],"frontMatter":{"slug":"tq-devlog-0","title":"PaaS devlog |#0","authors":["denis"],"tags":["paas"]},"unlisted":false,"prevItem":{"title":"Web API Design: A Simplified Approach","permalink":"/mynameis/blog/api-design"},"nextItem":{"title":"250mb json in a 40mb service limit","permalink":"/mynameis/blog/big-json"}},"content":"## Devlog #0: logging, linter, Zitadel, codegen, e2e tests\\n\\nToday I want to share with your my first steps of creating new project.\\nFor a long time I\'ve wanted created something cool, really meaninful, and after all I step into my idea: Platform as a service.\\n\\n\x3c!--truncate--\x3e\\n\\nThe main tech stack has beeen defined: Go for a main backend, perhaps JavaScript for cdk integration.\\nAll the infra will be used from a well known cloud providers (not aws ofc, no clue who can understand how to user it).\\nFirst iteration will not such words as a frontend, ui, design. Ofc I want to make it, but only future history can judge me.\\n\\n### The problem\\n\\nThe technical problem exists, it\'s fun to figure out.\\nBut no the main one. The biggest fight Im gonna accept is __procrastination__. Im such a person who pushes further all the tasks very often. I just have little to do with time discipline. That\'s what will stop me from the progress.\\nSo the easiest way to move progress away - do whatever, but not the progress.\\n\\n### The planing\\n\\nSo I decided to plan everything and understand how big the actual project is gonna be.\\nSo I start speaking with GPT in order to express my mind, kinda talking to a duck. \\nInstead of asking what I should do I explained my plan and asked to ask __more questions__ so it could help me to clarify the technical solution and the plan.\\n\\n![img](./gpt.png)\\n\\n<details>\\n<summary>\\nHere you can find a prompt I used\\n</summary>\\nI want to develop a platform like digitalocean app, vercel, render, fly.io, heroku, etc. My value is the following: I give observability with 0 code changes, and a localdev opportunity with connecting to a cloud environment, for instance a request comes from a frontend app on staging to my backend app locally so I could intercept it and make my laptop appear kinda in the cloud. It\'s a multitenant project. There are 2 paths: kubernetes and a custom architecture. To build a custom architecture the app must be on a few virtual machines, a load balancer on top including firewall. The challenge: ask me necessary questions so we could make a right decision. I have to highlight, we build a users\' environment, not the product architecture I will create in order to reproduce the given environment, so we need to decide that would fit me and users better.\\n</details>\\n\\nInitially I thought I will do it providing a user set of virtual machines connected under VPC, putting a load balancer on top, back to 2000. I thought it will make it cheaper and some tools just easier to implement.\\n\\nChatting to GPT I realized - Kubernetes will provide it a way quicker, most of the stuff is just ready to go like network policies, load balancing, SSL, deployment/rollback, health monitoring, resources observability, Istio adoption. Istio is very useful for observability and local first development, it became a new word today - __remocal__.\\n\\nWhen stepped into the planning I decided to make it even longer.\\nI wanted to researched all the kanban boards, unresistable.\\nI used to use Trello and it was ok, but using api in power-ups boundaries sucks. Not really somethign I need right now though. \\n\\nI tried Cluckup, it\'s a laggish joke, even worse than Jira to make you feel you are already enterprise.\\nI tried Asana and saw 0 difference with Trello, I spent about 2 minutes to confirm it and exist.\\n\\n### The ugly thing\\n\\nIn the end I found Linear. \\nTo be fair it looked ugly, the design is equal to supabase: fonts, buttons.\\n\\nA few minutes later I started catching.\\nKeyboard first design - that\'s what I buy.\\n\\nSo I read Linear Method, the framework they apply internally. It was hard to accept it, I think older I get more conservatibe I become, but I dig it anyway.\\nThe basic element is an issue. It might have sub-issues. And they might have.\\nThey can be collected in a project.\\n\\nThere are bunch of other staff I don\'t use, but I will share it below.\\n- __Cycle__ - like a sprint, hate it.\\n- __Triage__ - inbox, it\'s a boss feature in my opinion, you can aggregate communication channels in a single place, then an on-call person can carry them, create issues or remove.\\n- __Initiative__ - collection of the projects, they can be presented as a time line or a strategy decision, or even a roadmap on a timeline. \\n\\nAs a result I cooked quite a few cards (48 projects in a backlog \ud83d\ude31\ufe0f\ufe0f\ufe0f\ufe0f\ufe0f\ufe0f).\\n\\n![img](./few.png)\\n\\n## Infra preparation\\n\\n### Logging and recovery\\nI implemented a simple logging solution combined with a recovery middleware. \\nI find logging panic behaviors essential, and instead of separating these middlewares, I combined them for simplicity.\\n\\nThere are several ways to handle logging in Go applications:\\n- Inject a logger into every struct\\n- Define a global (singleton) logger\\n- Inject a logger into `context.Context`\\n\\n__I prefer the last approach__ because it offers the flexibility to:\\n- Add request IDs to log messages, useful when logging in the data layer\\n- Attach additional data fields to the logger for use in middleware, e.g. computed properties in a business layer\\n\\nThe implementation is straightforward, and you can find the code [here](https://github.com/treenq/treenq/blob/21dd255da33b7255ac1338edf75b36d048a6b2b7/pkg/vel/log/log.go#L72)\\n\\nA couple of _caveats_ I encountered:\\n1. There\u2019s a timestamp formatting issue in the slog package. It sometimes fails to format correctly, so I added a simple fix:\\n```go\\nif a.Value.Kind() == slog.KindTime {\\n  t := a.Value.Time()\\n  a.Value = slog.StringValue(t.Format(time.RFC3339))\\n}\\n```\\n2. I wanted to adjust the log level based on the HTTP response status: log warnings for client errors (4xx) and errors for server errors (5xx):\\n```go\\nlogFunc := logger.DebugContext\\nif resp.status >= 500 {\\n  logFunc = logger.ErrorContext\\n```\\n\\n3. Finally, I added panic recovery and logged the error:\\n```go\\ndefer func() {\\n  if recovered := recover(); recovered != nil {\\n    logger.ErrorContext(\\n      r.Context(), \\"recovered from panic\\",\\n      ...\\n      \\"recovered\\", recovered,\\n      \\"stack\\", string(debug.Stack()),\\n    )\\n    resp.WriteHeader(http.StatusInternalServerError)\\n    if _, err := resp.Write([]byte(\\"internal server error\\")); err != nil {\\n      logger.ErrorContext(r.Context(), \\"failed to write response\\", \\"error\\", err)\\n    }\\n  }\\n}()\\n```\\n\\nIt\u2019s crucial to override the HTTP status during recovery, which is why I had to implement my own response writer.\\n\\n### Linter\\nI kept my linter setup as simple as possible. \\nI don\u2019t use a complex configuration\u2014just a single command added to the Makefile:\\n```go\\ngo install github.com/golangci/golangci-lint/cmd/golangci-lint@v1.59.1\\n```\\n\\n### AuthZ\\n\\nMy goal was to define the simplest authentication solution, one that could be extended later to support role-based access control (RBAC).\\n\\nI considered a few options, including __Ory__ and __Keycloak__, but both felt like overkill for my needs\u2014too complex and cumbersome. \\nI wanted something:\\n- Easy to configure\\n- A single binary to allow self-hosting\\n- With a simple, clear open-source license (like Apache 2.0)\\n- Dependent only on PostgreSQL\\n\\nI initially looked at __Logto__, a fresh new solution that seemed simple and clean. \\nHowever, it felt geared more toward frontend developers rather than backend engineers, and lacked some of the features I needed.\\n\\nThat\u2019s when I found __Zitadel__. \\nCreated by some of the folks behind Ory, __Zitadel__ has strong security foundations. \\nSince I\u2019m building only a backend API, I focused on authZ, using token introspection (i.e., validation), rather than issuing tokens.\\n\\nIt was straightforward to create a service account token for my e2e tests to check the authorization middleware.\\n\\nI also verified that configuring RBAC and identity brokering with providers like Google and GitHub wouldn\u2019t be an issue (i.e., \u201cSign in with Google\u201d).\\n\\n###  Generate api client\\n\\nAs I explain in my API design [page](https://dennypenta.github.io/mynameis/blog/api-design) one of the benefits I want to achieve is easy API client generation for multiple languages. \\nThe first language, naturally, is Go, as I need it for my e2e tests. \\nSince the implementation will change frequently, I\u2019m skipping unit tests for now and focusing on e2e tests to ensure the app\u2019s overall stability.\\n\\nThe client generation involves:\\n- Collecting [API definition](https://github.com/treenq/treenq/blob/21dd255da33b7255ac1338edf75b36d048a6b2b7/pkg/vel/router.go#L124) from the router (models, routes)\\n- [Building](https://github.com/treenq/treenq/blob/21dd255da33b7255ac1338edf75b36d048a6b2b7/pkg/vel/gen/client.go#L35) the data from the definitions to use in the client generation.\\n- Feeding that data into a [template](https://github.com/treenq/treenq/blob/21dd255da33b7255ac1338edf75b36d048a6b2b7/pkg/vel/gen/templates/client.tpl#L1)\\n\\nThe result is a generated Go [client](https://github.com/treenq/treenq/blob/21dd255da33b7255ac1338edf75b36d048a6b2b7/client/client.go#L11). I can easily add other templates later, such as for TypeScript, and use them in my tests.\\n\\n### Defining e2e tests\\n\\nWith the generated client, I can now define the simplest setup for my e2e tests.\\n\\nFirst, I created a Docker image and a Docker Compose setup. \\nThese are similar to a regular production environment but with a few additional tricks.\\n\\nMy [Dockerfile](https://github.com/treenq/treenq/blob/21dd255da33b7255ac1338edf75b36d048a6b2b7/Dockerfile#L15) includes an extra [target](https://docs.docker.com/build/building/multi-stage/#stop-at-a-specific-build-stage) to run a debugger in the tests:\\n```js\\nFROM builder AS dev\\n\\n# Install Delve (debugger)\\nRUN --mount=type=cache,target=/go/pkg/mod/ --mount=type=cache,target=\\"/root/.cache/go-build\\" go install github.com/go-delve/delve/cmd/dlv@v1.23.0\\n\\nRUN --mount=type=cache,target=/go/pkg/mod/ --mount=type=cache,target=\\"/root/.cache/go-build\\" go build -gcflags=all=\\"-N -l\\" -o server ./cmd/server\\n\\nCMD [\\"dlv\\", \\"--listen=:40000\\", \\"--continue\\", \\"--headless=true\\", \\"--api-version=2\\", \\"--accept-multiclient\\", \\"exec\\", \\"server\\"]\\n```\\n\\nNext, I updated my [Docker Compose](https://github.com/treenq/treenq/blob/21dd255da33b7255ac1338edf75b36d048a6b2b7/docker-compose.e2e.yaml#L19) file to expose the debugger port and allow clients to attach:\\n```yaml\\nserver:\\n  build:\\n    context: .\\n    dockerfile: Dockerfile\\n    target: dev\\n  ports:\\n    - \'8000:8000\'\\n    - \'40000:40000\'\\n  security_opt:\\n    - seccomp:unconfined\\n  cap_add:\\n    - SYS_PTRACE\\n  env_file:\\n    e2e.env\\n  depends_on:\\n    postgrese2e:\\n      condition: service_healthy\\n  restart: always\\n```\\n\\nI also created a dedicated PostgreSQL instance for e2e tests to keep it separate from my local development environment:\\n```yaml\\n  postgrese2e:\\n    image: postgres:16.3\\n    restart: always\\n    environment:\\n      POSTGRES_HOST_AUTH_METHOD: \'trust\'\\n      POSTGRES_DB: tq\\n    ports:\\n      - \\"5432:5432\\"\\n    tmpfs:\\n      - /var/lib/postgresql/data\\n    healthcheck:\\n      test: [\\"CMD-SHELL\\", \\"pg_isready -U postgres -d tq\\"]\\n      interval: 3s\\n      timeout: 3s\\n      retries: 10\\n```\\n\\n\\nFinally, I defined a simple Makefile command to run the tests:\\n```js\\nstart-e2e-test-env:\\n\\tdocker-compose -f docker-compose.e2e.yaml up -d --build\\n\\t@echo \\"Checking e2e test environment is running...\\"\\n\\tuntil $$(curl --output /dev/null --silent --fail http://localhost:8000/healthz); do printf \'.\'; sleep 1; done && echo \\"Service Ready!\\"\\n\\techo \'Service has been started\'\\n```\\n\\nThe `curl` loop waits for the service to be ready since the app depends on the database connection, and migrations need to be applied. \\nThis kind of delay is common \u2014 you can imagine even more delays due to cache warming, downstream API fetching, etc.\\n\\n#### Bye\\n\\nThank you for reading, come here and see what\'s gonna be in the end.\\n\\nRepository is here: http://github.com/treenq/treenq"},{"id":"big-json","metadata":{"permalink":"/mynameis/blog/big-json","source":"@site/blog/02-250mbjson-in-50mb-limit/index.mdx","title":"250mb json in a 40mb service limit","description":"This article has been created to remind us of one simple thing: HTTP is a stream.","date":"2024-05-26T15:48:12.000Z","formattedDate":"May 26, 2024","tags":[{"label":"software","permalink":"/mynameis/blog/tags/software"}],"readingTime":6.8,"hasTruncateMarker":true,"authors":[{"name":"Denis","title":"Software Experience Dude","key":"denis"}],"frontMatter":{"slug":"big-json","title":"250mb json in a 40mb service limit","authors":["denis"],"tags":["software"]},"unlisted":false,"prevItem":{"title":"PaaS devlog |#0","permalink":"/mynameis/blog/tq-devlog-0"},"nextItem":{"title":"Why this page exists","permalink":"/mynameis/blog/why-a-blog"}},"content":"This article has been created to remind us of one simple thing: HTTP is a stream.\\n\\nAs a practical outcome we can learn how to reduce memory requirements for our services in a typical task: cache warming.\\n\\n\x3c!--truncate--\x3e\\n\\nLet\'s look at the challenge first.\\n\\nWe have a service that must download the data and keep it in memory.\\nThe issue is the JSON document we have to download is 10 times larger than the encoded data. \\nTherefore we have to increase the memory limit 2-3 times to download it once. \\nLater on, the service doesn\'t consume as much memory, so it\'s a start up cost.\\n\\n\\n### The challenge: cut down the memory consumption as much as we can.\\n\\nLet\'s get back to the basic of network communication.\\n\\n:::note\\nWe skip TLS termination for the sake of simplicity.\\n:::\\n\\nThere is a great book that explains it very well: https://hpbn.co/building-blocks-of-tcp/#slow-start\\n\\n![img](./syn.svg)\\n\\nJust a litle picture to remind us how a connection starts: we do a handshake with the service.\\n\\nThen we can start exchanging data.\\nTypical API responses are at most ~50kb.\\n\\nBut what if you want to warm a cache? How much can it be? \\nIt can be a lot, around tens of megabytes.\\nIn my example, we take 250mb.\\n\\nHow does the server send such data?\\n\\n![img](./congestion.svg)\\n\\nSlowly, packet by packet.\\n\\nThe server tries to understand your throughput. The protocol itself rarely provides an accurate value of a packet size, so by relying on the imperical latency, it tunes the packet size little by little.\\nIt needs to send a lot of packets to transfer a really big response.\\n\\n### 3 Ways to do it\\n\\nBelow we will consider 3 approaches to solve this task.\\nThere is no such thing as the only right solution; all of them are fine as long as you understand the costs and risks well enough, and we are gonna cover them.\\n\\n### First, Brute Force solution.\\n\\n:::warning\\nIf you want to reproduce an example make sure to untar server/json.tar.gz; it must contain the f.json file since GitHub has a limit of up to 100mb for a file.\\n:::\\n\\nYou can imagine how the simplest Go HTTP client can implement it or just look at the code.\\n\\n[Link](https://github.com/dennypenta/http-response-lab/blob/543510947c0b19dbc0097adf403ae5cd6954c1cc/client/main.go)\\n\\nThe implementation is straight forward: it sends a request, gets a response, reads, marhsals it into a defined structure, holds it in the memory and ready to serve it further.\\n\\nAnd here is the pprof output:\\n\\n```shell\\nShowing nodes accounting for 229.48MB, 100% of 229.48MB total\\n      flat  flat%   sum%        cum   cum%\\n  229.48MB   100%   100%   229.48MB   100%  io.ReadAll\\n         0     0%   100%   229.48MB   100%  main.main\\n         0     0%   100%   229.48MB   100%  runtime.main\\n```\\n\\nYes, it\'s not real win, with huge memory consumption, but it works ok.\\n\\nWe see all the memory consumed on reading the HTTP stream.\\n\\nOr you might say, \\"What a noob, you must use `json.Decoder`\\" so as to let the decoder work with the HTTP pipe closer.\\n\\nAnd it\'s pretty much the same, in my example, even worse.\\n[Link](https://github.com/dennypenta/http-response-lab/blob/b6ee7fcfd69fdffad844eb6a3d324d2fe3040985/client/main.go) to code with Decoder\\n\\n```shell\\nShowing nodes accounting for 384MB, 100% of 384MB total\\n      flat  flat%   sum%        cum   cum%\\n     384MB   100%   100%      384MB   100%  encoding/json.(*Decoder).refill\\n         0     0%   100%      384MB   100%  encoding/json.(*Decoder).Decode\\n         0     0%   100%      384MB   100%  encoding/json.(*Decoder).readValue\\n         0     0%   100%      384MB   100%  main.main\\n         0     0%   100%      384MB   100%  runtime.main\\n```\\n\\nTo recall why let\'s dig a little into the json/encoding library [implementation](https://cs.opensource.google/go/go/+/refs/tags/go1.22.3:src/encoding/json/stream.go;l=49). \\n\\n```go\\nfunc (dec *Decoder) Decode(v any) error {\\n\\tif dec.err != nil {\\n\\t\\treturn dec.err\\n\\t}\\n\\n\\tif err := dec.tokenPrepareForDecode(); err != nil {\\n\\t\\treturn err\\n\\t}\\n\\n\\tif !dec.tokenValueAllowed() {\\n\\t\\treturn &SyntaxError{msg: \\"not at beginning of value\\", Offset: dec.InputOffset()}\\n\\t}\\n\\n\\t// Read whole value into buffer.\\n\\tn, err := dec.readValue()\\n\\tif err != nil {\\n\\t\\treturn err\\n\\t}\\n\\tdec.d.init(dec.buf[dec.scanp : dec.scanp+n])\\n\\tdec.scanp += n\\n\\n\\t// Don\'t save err from unmarshal into dec.err:\\n\\t// the connection is still usable since we read a complete JSON\\n\\t// object from it before the error happened.\\n\\terr = dec.d.unmarshal(v)\\n\\n\\t// fixup token streaming state\\n\\tdec.tokenValueEnd()\\n\\n\\treturn err\\n}\\n```\\n\\nIt does exactly the same, it calls `dec.readValue()` first to read all the response and then `dec.d.unmarshal` to parse it.\\n\\nAnd it\'s ok; the reason is very simple: **encoding/json doesn\'t know the nature of your data.**\\n\\n:::note\\nLibraries like [json-iter](https://github.com/json-iterator/go) or [easyjson](https://github.com/mailru/easyjson) offer zero improvements in memory consumption.\\n:::\\n\\n### Second, decode object by object.\\n\\nGo json library provides a method of the Decoder called [`Token`](https://pkg.go.dev/encoding/json#Decoder.Token)\\n\\nThis approach, parsing manually token by token, can give us an option to manually parse the json.\\nA token might be every symbol, such as as open bracket, quote, key, value, etc.\\nBut I found this approach quite complex. Having a deeply nested JSON object makes it very confusing to understand the relation for a given token. An solution could be to hold every key and designated level, but the decision gets worse with duplicated keys on a couple of levels.\\n\\nThat\'s why I prefer another approach.\\n\\nThere is a well-known problem on LeetCode called \\"[Valid Parentheses](https://leetcode.com/problems/valid-parentheses/description/)\\"\\n\\nWe can simply read the beginning of a given object and the end, understanding when the last bracket of the object comes.\\n\\n[Link](https://github.com/dennypenta/http-response-lab/blob/b5890cbb74282416b5adacc92de95f18f7ee766f/client/main.go#L110)\\n\\npprof gives the following output\\n```shell\\n(pprof) top 5\\nShowing nodes accounting for 68.56MB, 100% of 68.56MB total\\nShowing top 5 nodes out of 10\\n      flat  flat%   sum%        cum   cum%\\n   27.55MB 40.18% 40.18%    68.56MB   100%  main.decode\\n      20MB 29.17% 69.35%       20MB 29.17%  encoding/json.(*decodeState).literalStore\\n      20MB 29.17% 98.52%       20MB 29.17%  bufio.NewReaderSize (inline)\\n    1.01MB  1.48%   100%     1.01MB  1.48%  bufio.(*Scanner).Text (inline)\\n         0     0%   100%       20MB 29.17%  encoding/json.(*decodeState).object\\n```\\n\\nUsually, the output varies between 65-80mb. \\n\\nSuch adventages is achieved due to marshalling the json **and** reading the HTTP response at the same time.\\n\\nLet\'s get back to the introduction. Such a huge response gives us a stream of HTTP chunks we read step by step until the FIN message comes.\\nWe can\'t make the HTTP server split every object in the response for us (probably we can, but it brings even more complexity).\\nInstead, every given chunk window we can ask, \\"Does it contain a valid json object?\\"\\n\\nAs soon as a valid object has come, we can marshal it and continue reading the response further **until the next valid JSON object comes**.\\n\\n### Third, the simplest: data compression.\\n\\nIt was new to me to discover that the most efficient solution will nott be related to the response handling.\\n\\n**We simply must transfer as little data as we can.**\\n\\nJSON is not the only way to represent the data.\\nIt\'s easy and human-readable, but sometimes we have to trade it.\\n\\nThere are plenty of formats we can apply:\\n- Avro\\n- Tthrift\\n- MessagePack\\n- Gob (Go only)\\n- Protobuf\\n\\nAnd most likely more I don\'t even know.\\n\\nI tried replacing decoding to MessagePack and gave very litle result (zero \\\\_(\u30c4)_/).\\n\\nThe best outcome showed Protobuf.\\n:::note\\nWe still use HTTP/1.1; we don\'t use gRPC transport.\\n:::\\n\\nThe output from pprof is even better with less effort to implement.\\nIt may vary up to 50mb sometimes.\\n\\n```shell\\n(pprof) top5\\nShowing nodes accounting for 30.79MB, 100% of 30.79MB total\\n      flat  flat%   sum%        cum   cum%\\n   30.79MB   100%   100%    30.79MB   100%  io.ReadAll\\n         0     0%   100%    30.79MB   100%  main.decode\\n         0     0%   100%    30.79MB   100%  main.main\\n         0     0%   100%    30.79MB   100%  runtime.main\\n```\\n\\nHere is the solution: [Link](https://github.com/dennypenta/http-response-lab/blob/main/client/main.go#L14)\\n\\nWhat we can say about Gob? \\n\\nIt offers very specific decoding and is a Go-only implementation, but it doesn\'t provide any benefit, here is the pprof output\\n```shell\\n(pprof) top5\\nShowing nodes accounting for 70.81MB, 100% of 70.81MB total\\nShowing top 5 nodes out of 20\\n      flat  flat%   sum%        cum   cum%\\n   30.53MB 43.12% 43.12%    30.53MB 43.12%  internal/saferio.ReadData\\n   28.28MB 39.94% 83.05%    28.28MB 39.94%  reflect.growslice\\n      12MB 16.95%   100%       12MB 16.95%  encoding/gob.decString\\n         0     0%   100%    70.81MB   100%  encoding/gob.(*Decoder).Decode\\n         0     0%   100%    70.81MB   100%  encoding/gob.(*Decoder).DecodeValue\\n```\\n\\nPerhaps for someone, having schemaless implementation is valuable, so here is the solution [link](https://github.com/dennypenta/http-response-lab/blob/main/client/main.go#L40)\\n\\n\\n### Conclusion\\n\\nI found 2 interesting ideas to me during the investigation.\\n1. The best, or one of them, solution might be the most obvious, so obvious to one is not to everyone.\\n2. It\'s not hard to implement and dig into fundamentals; some may win from engineering a new bicycle."},{"id":"why-a-blog","metadata":{"permalink":"/mynameis/blog/why-a-blog","source":"@site/blog/01-why-you-need-a-blog/index.mdx","title":"Why this page exists","description":"Blog is a\xa0 system to spread ideas across the internet. Shout out about a thing \\"Look, I have an opinion on that if you care\\".","date":"2023-12-13T21:25:34.000Z","formattedDate":"December 13, 2023","tags":[{"label":"career","permalink":"/mynameis/blog/tags/career"}],"readingTime":3.305,"hasTruncateMarker":true,"authors":[{"name":"Denis","title":"Software Experience Dude","key":"denis"}],"frontMatter":{"slug":"why-a-blog","title":"Why this page exists","authors":["denis"],"tags":["career"]},"unlisted":false,"prevItem":{"title":"250mb json in a 40mb service limit","permalink":"/mynameis/blog/big-json"}},"content":"Blog is a\xa0 system to spread ideas across the internet. Shout out about a thing \\"Look, I have an opinion on that if you care\\".\\n\\n\x3c!--truncate--\x3e\\n\\nI have opinion on different things, especially in software. I\'ve been working on it since 2015.  \\n\\nSo let\'s start from reasoning why you might want to own one.\\n\\n## It makes more than 0 chances somebody will find it and share your opinion\\n\\nOr disagree, it\'s also fine and in some cases means almost the the same for you.\\n\\nYou get audience.\xa0\\n\\nAnd it\'s a big deal.\\n\\nFirst, the audience challenges your ideas. It\'s good to find a person to discuss your ideas. It allows your either find weaknesses in the idea itself, or perhaps your mindset to find crucial vocabulary to express what it means for you, or become even more confident on a topic. After all, it\'s just a joy to discuss what you have on top of your head.\\n\\n## More than 0 chances your future team knows you\\n\\nIt\'s a well known that having a referral in a company makes hiring easier to everyone. They get plus a point in confidence they hire not a dumbass, you may skip long questionnaire. They refer you not because you are good at contribution, but it\'s easier to build ubiquitous language in the team when you know each other.\\n\\nI want to say having written something valuable for others makes you a little more known that you used to be. And if you apply to a new job and they have found it - it bumps your chances a lot.\\n\\nIm convinced in the idea that not just delivering good result to your company, but telling the best pieces to the others will pay me off later on.\\n\\nThe only thing is under question if we really have built and understood something important for the industry.\\n\\n## Memo is good\\n\\nDuring software experience I realised the wide range technologies I apply can hardly be stored in my head.\\n\\nTools such Obsidian are fancy, but Im not disciplined enough to handle my cloud to store it or just structure them well enough, it doesn\'t fit me.\\n\\nSo Im going to structure my experience here.\\n\\nIt\'s not just putting all I have in my memory, but also try structuring all new stuff I have in my head.\\n\\nFor instance, I want to memoize a way to define alerts using terraform. Even though I\'ve done it I have no idea what those queries to an observability provider mean.\xa0\\n\\nWhat should I do?\\n\\nNot to fool people. I will figure out in order to explain it to the others. Otherwise the article will be one of the thousands on medium or linkedin.\\n\\nEventually I have to push my skill up to give my audience valuable experience shared.\\n\\nIn the same time I get structured my experience and making it better by reading more in order to write a little.\\n\\n## Expressing better\\n\\nAnd my personal reason. \\n\\nIm poor at structuring my speach. If you read it you know how hard it was to catch my ideas. But I fight against it and I believe the blog may make me write more often and amend the way I express myself.\\n\\n## You are ready. what now?\\n\\nOk, let\'s say you confirmed you have a reason to have a blog.\\n\\n  \\n\\nA couple of hints I want to add.\\n\\n  \\n\\nPlatforms like medium blocks people from reading your content without subscription. \\nAnd definitely it\'s not what I want.\\n\\nI can understand if you want to setup patreon to provide sub only data, but I don\'t consider such option.\xa0\\nMoreover, the platforms get bigger revenue chunk of your ad and don\'t do more than google does.\\n\\nI picked docusaurus because I have full control on codebase, fonts, styles, ads, comments, structuring. And I was lucky enough to find a template I wanted to reuse by [takken](https://takken.io/).\\n\\nOr you can just clone this [project](https://github.com/dennypenta/mynameis)."}]}')}}]);